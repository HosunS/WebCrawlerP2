¦Ifinal_url¡DtypeLhttp_headers¢DtypeEvalue‡¢Ak¢DtypeEvalueNContent-LengthAv¢DtypeEvalueE13769¢Ak¢DtypeEvalueMAccept-RangesAv¢DtypeEvalueEbytes¢Ak¢DtypeEvalueFServerAv¢DtypeEvalueX4Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips SVN/1.7.14¢Ak¢DtypeEvalueMLast-ModifiedAv¢DtypeEvalueXMon, 28 Jan 2019 18:39:43 GMT¢Ak¢DtypeEvalueDETagAv¢DtypeEvalueT"35c9-580890012f987"¢Ak¢DtypeEvalueDDateAv¢DtypeEvalueXSun, 03 Feb 2019 02:36:39 GMT¢Ak¢DtypeEvalueLContent-TypeAv¢DtypeEvalueXtext/html; charset=UTF-8Kraw_content¢DtypeEvalueY5É<!DOCTYPE html>
<html>
<title>CS 274A | Notes/Reading</title>
<meta name="viewport" content="width=device-width, initial-scale=1">


<!--MATERIAL BELOW TO BE INCLUDED ON ALL SITE PAGES--> 
<link rel="stylesheet" href="https://www.ics.uci.edu/~smyth/test/localw3.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css"> <!--mini icons--> 
<link rel="stylesheet" href="http://www.w3schools.com/lib/w3-theme-blue.css">  <!-- color theme -->
<link rel="stylesheet" type="text/css" href="localstyle.css">  <!-- my own style -->
<link rel="stylesheet" type="text/css" href="https://www.ics.uci.edu/~smyth/courses/class_style.css">  <!-- Padhraic's class style --->  
<link rel="icon" type="image/png" href="https://news.uci.edu/wp-content/uploads/2014/12/favicon.png"/> <!-- favicon -->

<body class="w3-animate-opacity">
  
<div style="margin-left:10%;  margin-right:10%;  margin-top:30px">  
 
  

<!-- -------------------------------------------------------------------------------------------- -->


 
<h4> <span style="font-weight: bold;"> CS 274A: Background Notes and Reading, Winter 2019 </span></h4> 

Note that the contents of this page may be updated periodically during the quarter.


<p>
<h6><b>Notes</b> (Note Sets 1 to 3 are particularly relevant for the 1st and 2nd week)</h6>
<ul  style="list-style-type:disc">
<li><a href="notes/notation.pdf">Notation Guide</a> </li>
<li><a href="notes/notes1.pdf">Note Set 1: Review of Probability (PDF)</a></li>
<li><a href="notes/notes2.pdf">Note Set 2: Multivariate Probability Models (PDF)</a></li>
<li><a href="notes/notes3.pdf">Note Set 3: Models, Parameters, and Likelihood (PDF)</a></li>
<li><a href="notes/notes_Bayesian.pdf">Notes on Bayesian Estimation (PDF)</a></li>
<li><a href="notes/mixture_models_EM.pdf">Notes on Mixture Models and the EM Algorithm (PDF)</a></li>
<li><a href="notes/hmm_notes.pdf">Notes on Hidden Markov Models (PDF)</a> </li>
<li><a href="notes/notes_optimal_classification.pdf">Notes on Discriminant Functions and Optimal Classification (PDF)</a> </li>
</ul>
</p> 
  


<p>
<h6><b>Textbooks</b> (recommended for background reading but not required)</h6>
<ul>
<li> <a href="http://www.deeplearningbook.org/"><b>Deep Learning</b></a>, by Goodfellow, Bengio, and Courville, Bengio, MIT Press, 2016.
Even though this text is mostly about deep learning (Sections II and III, and beyond the scope of our class),
 Section I is about probabilistic
learning in general and provides a lot of useful background material for this class. 
</li>
<li> 
<a href="http://www.cs.ubc.ca/~murphyk/MLbook/index.html"><b>Machine Learning: A Probabilistic Perspective</b></a>, 
by Kevin Murphy, MIT Press, 2012. The current standard reference text for probabilistic machine learning. Covers
far more than we will cover in this 10-week class.
If you plan to use machine learning in your research after this class you may want to buy
a copy of this text - you will find it to be a very useful reference in your research.
</li>
<li><a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online"><b>Bayesian Reasoning and Machine Learning</b></a>, 
by David Barber, Cambridge University Press. Another useful reference text on probabilistic learning (the PDF version is free).
<li><a href="https://www.nature.com/articles/nature14541"><b>Probabilistic machine learning and artificial intelligence</b></a>, Zoubin Ghahramani, <i> Nature</i>, 2015. Good overview article of the role of probability in modern machine learning and AI.
</li>
<li><a href="readings/Bishop-MBML-2012.pdf"><b>Model-based machine learning</b></a>, Chris Bishop, 
<i>Phil Trans R. Soc. A</i>, 2012. 
A well-written overview article that reviews some of the key ideas behind probabilistic model-based learning</li>
</li>
</ul>
</p>  
 
 
<br>
<b> General Background/Review Material on Probability </b>:  
<ul>
<li> Topics covered: random variables, conditional and joint 
probabilities, Bayes rule, law of total probability, chain rule 
and factorization. Frequentist and Bayesian views of probability. Sets of random variables, the 
multivariate Gaussian model. Conditional independence and graphical 
models. Markov models. </li>
<li> Required Reading: Note Sets 1 and 2 above </li>
<li> Recommended Additional Reading 
<ul>
<li> Goodfellow et al text: <a href="http://www.deeplearningbook.org/contents/prob.html">Chapter 3, Probability and
Information Theory</a>  (well worth reading before doing Homework 1)
<li>  Barber text: Chapter 1.1, 1.2 (basic probability), Chapter 2 (graphs), Chapter 3.1 to 3.3 (directed
graphical models, referred to here as 'belief networks'), sections 8.1 to 8.4 (univariate and
multivariate distributions) </li>
<li> Class notes from  Kevin
Murphy on <a href="readings/Murphy_directed_graphical_models.pdf"> on directed graphical
models</a>, <a href="readings/Murphy_MarkovModels.pdf"> Markov models</a>, and <a
href="readings/Murphy_multivariate_gaussians.pdf"> multivariate Gaussians</a>  </li>
</li>
</ul>
<li> Optional Reading
<ul>
<li> Murphy text:
Chapter 1 (introduction), Chapter 2.1 through 2.5 &nbsp;(probability and distributions), Chapter
10.1 to 10.3 (graphical models) </li>
<li> Excellent 15 minute <a
href="http://www.youtube.com/watch?v=eho8xH3E6mE&amp;feature=youtu.be">video on multivariate
Gaussian distributions</a> from our own Alex Ihler</li>
<li> Chapter from Chris Bishop's book on <a
href="readings/Bishop_graphical_models_chapter.pdf">graphical models</a> (the material on graphical models starts about 20 pages into the document)</li>
</ul></li>
</ul> 
 
<br>
<b> Learning from Data using Maximum Likelihood </b> <br>
<ul>
<li> Topics: Concepts of models and
parameters. Definition of the likelihood function and the principle of maximum likelihood
parameter estimation. Using maximum likelihood methods to learn the parameters of Gaussian models, binomial,
multivariate and other parametric models.</li>
<li> Required Reading: Note Set 3 above </li>
<li> Recommended Additional Reading 
<ul>
<li> Barber text: pages 174-177 </li>
<li> <a href="readings/maximum_likelihood_tutorial.pdf">Tutorial  paper on maximum likelihood estimation</a> </li>
</ul></li>
</ul> 


<br>
<b> Bayesian Learning </b> <br>
<ul>
<li> Topics: General principles of Bayesian estimation: prior densities, posterior densities, MAP, 
fully Bayesian approaches. Beta/binomial and Gaussian examples. Predictive densities, model selection, model averaging.</li>
<li> Required Reading: Review Note Sets 1 and 2 again </li>
<li> Recommended Reading:
<ul>
<li> Notes on <a href="readings/Murphy_Bayesian_notes.pdf"> analysis of binomial and multinomial models</a> from Kevin Murphy </li>
<li> Barber text: pages 191-194 (in Chapter 9, Learning as Inference), Pages 177-179 in Chapter 8  </li>
</ul>
</li>
<li> Optional Additional Reading:
<ul>
<li> Barber text: Chapter 12 on Bayesian Model Selection
<li> Murphy text: Chapter 3.1 to 3.4 and Chapter 5.1, 5.2, 5.3 </li>
<li> An introductory chapter on the <a href="http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/47.59.pdf">
principles of Bayesian inference </a> by the late David Mackay, from
his excellent book <a href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html">
Information Theory, Inference, and Learning Algorithms</a>. Also a link to a video of David
lecturing on <a href="http://videolectures.net/mackay_course_10/">An Introduction to Bayesian
Inference</a>. </li>
</ul> </li>
</ul>


<br>
<b> Optimization Methods for Machine Learning </b> <br>
<ul>
<li> Topics: General principles of finding minima/maxima of multivariate functions,  gradient and Hessian methods, 
stochastic gradient methods.
</li> 
<li>  Recommended Reading:
<ul>
<li> Geoff Hinton's class notes on 
<a href="readings/optimization/hinton.pdf">optimization for machine learning </a> (a good introduction to the basic concepts)</li>
<li> Chapter on <a href="http://www.deeplearningbook.org/contents/numerical.html"> Numerical Optimization</a> from
the Goodfellow et al. text. </li>
<li> Tutorial paper from Leon Bottou on 
<a href="readings/optimization/bottou.pdf">stochastic gradient methods</a>.</li>
</ul>
</li>

<li> Optional Additional Reading:
<ul>
<li> Nice<a href="http://www.statisticsviews.com/details/feature/5722691/Getting-to-the-Bottom-of-Regression-
with-Gradient-Descent.html"> online tutorial on gradient descent</a>, with examples in R code.</li>
<li>  Notes on <a href="readings/optimization/shewchuk.pdf"> conjugate gradient descent</a>, from Jonathan
Shewchuk (with good insights into geometric aspects of optimization in general) </li>
<li> Slides from Stephen Wright optimization and machine learning:
<a href="http://helper.ipam.ucla.edu/publications/gss2012/gss2012_10763.pdf">IPAM 2012 slides</a>
and <a href="http://pages.cs.wisc.edu/%7Eswright/nips2010/sjw-nips10.pdf">NIPS 2010
slides</a> (see also
<a href="http://videolectures.net/nips2010_wright_oaml/">video</a>). </li>
</ul></li>
</ul> 


<br>
<b> Regression Models </b> <br>
<ul>
<li> Topics: Linear models. Normal equations. Systematic and stochastic components. 
Parameter estimation methods for regression. Maximum likelihood and Bayesian
interpretations. </li>
<li> Recommended Reading:
<ul>
<li> <a href="readings/andrew_ng_notes.pdf">Andrew Ng's notes on supervised learning</a> (good
introduction to the basic concepts) </li>
<li>Barber text: Chapter 17.1 to 17.3 on linear models</li>
<li> Nice <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">blog post
on key ideas associated with the bias-variance trade-off</a></li>
<li> <a href="readings/xing_singh_CMU_bias_variance.pdf">Slides from CMU on
bias-variance</a></li>
</ul>
</li>
<li> Optional Additional Reading:
<ul>
<li>Murphy text: Chapter 7.1, 7.2, 7.3 and 7.6</li>
<li> <a href="http://www.deeplearningbook.org/contents/ml.html">
General discussion of classification and regression</a> from Goodfellow et al. text </li>
<li>pages 1 to 33 of <a href="readings/bias_variance.pdf">a classic paper
on the bias/variance tradeoff</a> </li>
<li> <a href="readings/bayesian_regression_overview.pdf">Mike Tipping's review paper on Bayesian
regression</a></li>
</ul></li>
</ul>


<br>
<b> Probabilistic Classification </b> <br>
<ul>
<li> Topics: Bayes rule, classification boundaries, discriminant functions, Optimal decisions, 
Bayes error rate, Gaussian classifiers. Likelihood-based approaches and properties of 
objective functions. Logistic regression and neural network models.  </li>
<li> Required Reading: <a href="notes/notes_optimal_classification.pdf">Notes on Discriminant Functions and Optimal Classification (PDF)</a>
<li> Recommended Reading:
<ul>
<li> Barber text: pages 229-234 (in Chapter 10 on Naive Bayes), 
pages 353-358 on logistic regression (in Chapter 17 on Linear Models) </li>
<li> <a href="readings/elkan_logisticregression_notes.pdf">Notes on logistic regression</a> 
from Charles Elkan </li>
</ul>
</li>
<li> Optional Additional Reading:
<ul>
<li> Murphy text: pages 101-107 on Gaussian classifiers in Chapter 4, Chapter 8.1. 8.2, 8.3  </li>
<li> Paper on <a href="http://www.stat.columbia.edu/%7Emadigan/PAPERS/techno.pdf">
Logistic regression for high-dimensional text data</a> </li>
</ul></li>
</ul>


<br>
<b> The EM Algorithm, Mixture Models, and Probabilistic Clustering </b> <br>
<ul>
<li> Topics: Mixtures of Gaussians and the associated EM algorithm.  
K-means clustering. Mixtures of conditional indepedence models.  
Applications to text data. Underlying theory of the EM algorithm. </li>
<li> Required Reading: Note Set 4 above (EM for Gaussian mixture models) </li>
<li> 
<a href="readings/domke_notes_on_EM.pdf">Notes on derivation of EM algorithm</a> from Justin Domke</li>
<li> General background on mixtures and EM:  Barber text, pages 403-416; Murphy text: pages 337-356 (Chapter 11) </li>
<li> General derivation of the EM Algorithm: pages 404-406 in Barber, pages 363-369 in Murphy </li> 
<li> 
<a href="readings/blei_mixtures-and-gibbs"> Bayesian learning of mixtures, with Gibbs sampling</a> from Dave Blei
<li> Jeff Bilmes 
<a href="http://ssli.ee.washington.edu/people/bilmes/mypapers/em.pdf">tutorial notes on EM </a></li>
<li> Frank Dellaert's <a href="http://www.cc.gatech.edu/%7Edellaert/em-paper.pdf">tutorial notes on EM</a> </li>
<li> Liang and Klein's<a href="http://dl.acm.org/citation.cfm?id=1620843"> Online EM
with applications to text</a> </li>
<li> Fraley and Raftery <a href="readings/fraley_raftery.pdf">paper on model-based clustering </a> </li>
</ul>


<br>
<b> State-Space and Time-Series Models </b> <br>
<ul>
<li> Topics: discrete and continuous latent-state space models. Hidden Markov models, 
Kalman filters. Basic principles of smoothing and filtering. Parameter estimation methods using EM.  </li>
<li> Required Reading: Note Set 5 above on Hidden Markov models.
<li> 
<a href="http://web4.cs.ucl.ac.uk/staff/d.barber/publications/GMSPM.pdf">
Tutorial paper on latent-variable models for time-series data</a>, Barber and Cemgil, 
<i>IEEE Signal Processing Magazine</i>, 2010.
<li> Barber text: pages 451-471 (in Chapter 23 on Dynamical Models) </li>
<li> Murphy text: Chapter 17.1 to 17.5 </li>
<li> Sequential modeling using <a href="http://www.deeplearningbook.org/contents/rnn.html">
recurrent neural networks </a> from the Goodfellow et al. text</li>
</ul>

<br>
<b> Sampling Methods </b> <br>
<ul>
<li> Topics: Importance sampling, Gibbs sampling, and related ideas </li>
<li> <a href="http://www.deeplearningbook.org/contents/monte_carlo.html">Tutorial chapter
on sampling and Monte Carlo methods</a> from the Goodfellow et al. text</li>
<li> Barber text: pages 543-553 (in Chapter 27 on Sampling)  </li>
</ul>


</body>
</html>

 


</body>Mis_redirected¢DtypeEvalueôIhttp_code¢DtypeEvalueÈQdownload_complete¢DtypeEvalueõ
¶Ifinal_url¢DtypeEvaluexhttps://cml.ics.uci.edu/2018/Lhttp_headers¢DtypeEvalueá¢Ak¢DtypeEvalueQTransfer-EncodingAv¢DtypeEvalueGchunked¢Ak¢DtypeEvalueJKeep-AliveAv¢DtypeEvalueQtimeout=5, max=99¢Ak¢DtypeEvalueFServerAv¢DtypeEvalueX4Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips SVN/1.7.14¢Ak¢DtypeEvalueJConnectionAv¢DtypeEvalueJKeep-Alive¢Ak¢DtypeEvalueDLinkAv¢DtypeEvalueX<<https://cml.ics.uci.edu/wp-json/>; rel="https://api.w.org/"¢Ak¢DtypeEvalueDDateAv¢DtypeEvalueXMon, 18 Feb 2019 11:17:34 GMT¢Ak¢DtypeEvalueLContent-TypeAv¢DtypeEvalueXtext/html; charset=UTF-8Kraw_content¢DtypeEvalueYÏ†<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />
<title>2018 | Center for Machine Learning and Intelligent Systems</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://cml.ics.uci.edu/xmlrpc.php" />
<!--[if lt IE 9]>
<script src="https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-wpcom/js/html5.js" type="text/javascript"></script>
<![endif]-->

<link rel='dns-prefetch' href='//s.w.org' />
<link rel="alternate" type="application/rss+xml" title="Center for Machine Learning and Intelligent Systems &raquo; Feed" href="https://cml.ics.uci.edu/feed/" />
<link rel="alternate" type="application/rss+xml" title="Center for Machine Learning and Intelligent Systems &raquo; Comments Feed" href="https://cml.ics.uci.edu/comments/feed/" />
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/cml.ics.uci.edu\/wp-includes\/js\/wp-emoji-release.min.js?ver=5.0.2"}};
			!function(a,b,c){function d(a,b){var c=String.fromCharCode;l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,a),0,0);var d=k.toDataURL();l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,b),0,0);var e=k.toDataURL();return d===e}function e(a){var b;if(!l||!l.fillText)return!1;switch(l.textBaseline="top",l.font="600 32px Arial",a){case"flag":return!(b=d([55356,56826,55356,56819],[55356,56826,8203,55356,56819]))&&(b=d([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]),!b);case"emoji":return b=d([55358,56760,9792,65039],[55358,56760,8203,9792,65039]),!b}return!1}function f(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var g,h,i,j,k=b.createElement("canvas"),l=k.getContext&&k.getContext("2d");for(j=Array("flag","emoji"),c.supports={everything:!0,everythingExceptFlag:!0},i=0;i<j.length;i++)c.supports[j[i]]=e(j[i]),c.supports.everything=c.supports.everything&&c.supports[j[i]],"flag"!==j[i]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[j[i]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(h=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",h,!1),a.addEventListener("load",h,!1)):(a.attachEvent("onload",h),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),g=c.source||{},g.concatemoji?f(g.concatemoji):g.wpemoji&&g.twemoji&&(f(g.twemoji),f(g.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel='stylesheet' id='wp-block-library-css'  href='https://cml.ics.uci.edu/wp-includes/css/dist/block-library/style.min.css?ver=5.0.2' type='text/css' media='all' />
<link rel='stylesheet' id='bonpress-style-css'  href='https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-cml/style.css?ver=5.0.2' type='text/css' media='all' />
<link rel='stylesheet' id='tipsy-css'  href='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/css/tipsy.css?ver=5.0.2' type='text/css' media='all' />
<link rel='stylesheet' id='mts_wpshortcodes-css'  href='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/css/wp-shortcode.css?ver=5.0.2' type='text/css' media='all' />
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/jquery/jquery.js?ver=1.12.4'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/jquery/jquery-migrate.min.js?ver=1.4.1'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/js/jquery.tipsy.js?ver=5.0.2'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/js/wp-shortcode.js?ver=5.0.2'></script>
<link rel='https://api.w.org/' href='https://cml.ics.uci.edu/wp-json/' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://cml.ics.uci.edu/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://cml.ics.uci.edu/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 5.0.2" />
</head>

<body class="archive date group-blog">
<div id="page" class="hfeed site">
		<header id="masthead" class="all-header" role="banner">
		<hgroup class="hgroup-wide">
                        <a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home"><img src='/wp-content/cml/uploads/cml-curve.jpg'></a>
<!--			<h1 class="site-title"><a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home">Center for Machine Learning and Intelligent Systems</a></h1>
			<h2 class="site-description">University of California, Irvine</h2> -->
			<h1 class="site-title"><a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home">Center for Machine Learning and Intelligent Systems</a></h1>
			<h2 class="site-description">Bren School of Information and Computer Science</h2>			
			<h2 class="site-description">University of California, Irvine</h2>
			<div style="clear:both"></div>
		</hgroup>
	</header>
	<header id="masthead" class="site-header" role="banner">
		<hgroup class="hgroup-img">
                        <a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home"><img src='/wp-content/cml/uploads/cml-curve.jpg'></a>
			<h1 class="site-title"><a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home">Center for Machine Learning and Intelligent Systems</a></h1>
			<h2 class="site-description">University of California, Irvine</h2>
		</hgroup>

		<nav id="site-navigation" class="navigation-main" role="navigation">
			<h1 class="menu-toggle">Menu</h1>
			<div class="screen-reader-text skip-link"><a href="#content" title="Skip to content">Skip to content</a></div>

			<div class="menu-navigation-container"><ul id="menu-navigation" class="menu"><li id="menu-item-234" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-234"><a href="https://cml.ics.uci.edu/">Home</a></li>
<li id="menu-item-79" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-79"><a href="https://cml.ics.uci.edu/home/about-us/">About CML</a>
<ul class="sub-menu">
	<li id="menu-item-78" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-78"><a href="https://cml.ics.uci.edu/home/about-us/">About us</a></li>
	<li id="menu-item-429" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-429"><a href="https://cml.ics.uci.edu/category/news/">News</a></li>
	<li id="menu-item-76" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-76"><a href="https://cml.ics.uci.edu/home/contact-us/">Contact Us</a></li>
</ul>
</li>
<li id="menu-item-539" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-539"><a>People</a>
<ul class="sub-menu">
	<li id="menu-item-55" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-55"><a href="https://cml.ics.uci.edu/faculty/">Faculty</a></li>
	<li id="menu-item-220" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-220"><a href="https://cml.ics.uci.edu/alumni/">Alumni</a></li>
</ul>
</li>
<li id="menu-item-75" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-75"><a href="https://cml.ics.uci.edu/aiml/">Events &#038; Seminars</a>
<ul class="sub-menu">
	<li id="menu-item-74" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-74"><a href="https://cml.ics.uci.edu/aiml/">AI/ML Seminar Series</a></li>
	<li id="menu-item-73" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-73"><a href="https://cml.ics.uci.edu/aiml/ml-reading-group/">ML Reading Group</a></li>
</ul>
</li>
<li id="menu-item-222" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-222"><a>Education &#038; Resources</a>
<ul class="sub-menu">
	<li id="menu-item-227" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-227"><a href="https://cml.ics.uci.edu/courses/">Courses</a></li>
	<li id="menu-item-221" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-221"><a href="https://cml.ics.uci.edu/books/">Books</a></li>
</ul>
</li>
<li id="menu-item-81" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-81"><a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI Machine Learning Archive</a></li>
<li id="menu-item-87" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-87"><a href="https://cml.ics.uci.edu/sponsors-funding/">Sponsors &#038; Funding</a></li>
<li id="menu-item-86" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-86"><a href="https://cml.ics.uci.edu/subscribe/">Subscribe to CML List</a></li>
</ul></div>		</nav><!-- #site-navigation -->
	</header><!-- #masthead -->


	<section id="primary" class="content-area">
		<div id="content" class="site-content" role="main">

		
			<header class="page-header clear">
				<h1 class="page-title">
					<span>2018</span>				</h1>
							</header><!-- .page-header -->

						
				
<article id="post-822" class="post-822 post type-post status-publish format-standard hentry category-news">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2018/10/faculty-positions-at-uc-irvine/" title="Permalink to Faculty Positions at UC Irvine" rel="bookmark">Faculty Positions at UC Irvine</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		<p>Faculty Positions at UC Irvine</p>
<p>Application deadline: Jan 15th, 2019 (Applications received by January 1, 2019 will receive fullest consideration.)</p>
<p>Apply online at:  <a href="https://recruit.ap.uci.edu/apply/JPF04950">https://recruit.ap.uci.edu/apply/JPF04950</a></p>
<p>The Department of Computer Science in the Donald Bren School of Information and Computer Sciences (ICS) at the University of California, Irvine (UCI) invites applications for multiple tenure-track assistant professor or tenured associate/full professor positions beginning July 1, 2019. The Department is interested in individuals with research interests in all aspects of algorithms, artificial intelligence, machine learning, and theory of computing. One opening is targeted at individuals whose computer science expertise aligns with the growing UCI Data Science Initiative.</p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2018/10/faculty-positions-at-uc-irvine/" title="2:29 pm" rel="bookmark"><time class="entry-date genericon" datetime="2018-10-12T14:29:30+00:00">October 12, 2018</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-811" class="post-811 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2018/09/fall-2018/" title="Permalink to Fall 2018" rel="bookmark">Fall 2018</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		<p><!-- ========================================================== --><br />
<!-- =====Fall Quarter ============================================== --><br />
<!-- ========================================================== --></p>
<table border="1" cellpadding="5"><!-- ==== Oct 1 =================================== --></p>
<tbody>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>Oct 1</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>No Seminar</b></div>
<p>&nbsp;</td>
</tr>
<p><!-- ==== Oct 8 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Oct 8</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><a href="https://allenai.org/team/mattg/"><b>Matt Gardner</b></a><br />
Research Scientist<br />
Allen Institute for AI</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>A Tale of Two Question Answering Systems</b></a></span></div><div class="togglec clearfix">The path to natural language understanding goes through increasingly challenging question answering tasks. I will present research that significantly improves performance on two such tasks: answering complex questions over tables, and open-domain factoid question answering. For answering complex questions, I will present a type-constrained encoder-decoder neural semantic parser that learns to map natural language questions to programs. For open-domain factoid QA, I will show that training paragraph-level QA systems to give calibrated confidence scores across paragraphs is crucial when the correct answer-containing paragraph is unknown. I will conclude with some thoughts about how to combine these two disparate QA paradigms, towards the goal of answering complex questions over open-domain text.</p>
<p><b>Bio:</b>Matt Gardner is a research scientist at the Allen Institute for Artificial Intelligence (AI2), where he has been exploring various kinds of question answering systems. He is the lead designer and maintainer of the AllenNLP toolkit, a platform for doing NLP research on top of pytorch. Matt is also the co-host of the NLP Highlights podcast, where, with Waleed Ammar, he gets to interview the authors of interesting NLP papers about their work. Prior to joining AI2, Matt earned a PhD from Carnegie Mellon University, working with Tom Mitchell on the Never Ending Language Learning project.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Oct 15 =================================== --></p>
<p><!-- ==== Oct 22 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Oct 22</b></div>
<div>
<div></div>
</div>
<div>Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><a style="font-family: inherit; font-size: inherit;" href="http://www.stephanmandt.com/"><b>Stephan Mandt</b></a></div>
</div>
<div class="aiml-name">
<div class="aiml-name">Assistant Professor<br />
Dept. of Computer Science<br />
UC Irvine</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Deep Probabilistic Modeling</b></a></span></div><div class="togglec clearfix">I will give an overview of some exciting recent developments in deep probabilistic modeling, which combines deep neural networks with probabilistic models for unsupervised learning. Deep probabilistic models are capable of synthesizing artificial data that highly resemble the training data, and are able fool both machine learning classifiers as well as humans. These models have numerous applications in creative tasks, such as voice, image, or video synthesis and manipulation. At the same time, combining neural networks with strong priors results in flexible yet highly interpretable models for finding hidden structure in large data sets. I will summarize my group‚Äôs activities in this space, including measuring semantic shifts of individual words over hundreds of years, summarizing audience reactions to movies, and predicting the future evolution of video sequences with applications to neural video coding.</div></div><div class="clear"></div>
</div>
</tr>
<p><!-- ==== Oct 25 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Oct 25</b><br />
Bren Hall 3011<br />
3 pm</div>
</td>
<td valign="top">
<div class="aiml-name">
<div class="aiml-name">
<div class="aiml-name"></div>
<div></div>
</div>
</div>
<div class="aiml-name">
<div><strong>(Note: different day (Thurs), time (3pm), and location (3011) relative to usual Monday seminars)</p>
<p></strong></div>
<div><a href="http://pages.cs.wisc.edu/~swright/"><b>Steven Wright</b></a><br />
Professor<br />
Department of Computer Sciences<br />
University of Wisconsin, Madison</div>
</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Optimization in Data Science</b></a></span></div><div class="togglec clearfix">Many of the computational problems that arise in data analysis and<br />
machine learning can be expressed mathematically as¬†<span class="m_-567682425052202021gmail-il">optimization¬†</span>problems. Indeed, much new algorithmic research in¬†<span class="m_-567682425052202021gmail-il">optimization</span>¬†is being driven by the need to solve large, complex problems from these areas. In this talk, we review a number of canonical problems in data analysis and their formulations as¬†<span class="m_-567682425052202021gmail-il">optimization</span>¬†problems. We will cover support vector machines / kernel learning, logistic regression (including regularized and multiclass variants), matrix completion, deep learning, and several other paradigms.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Oct 29 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Oct 29</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><a href="http://www.cs.cmu.edu/~cpsomas/"><b>Alex Psomas</b></a><br />
Postdoctoral Researcher<br />
Computer Science Department<br />
Carnegie Mellon University</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Fair Resource Allocation: From Theory to Practice</b></a></span></div><div class="togglec clearfix">We study the problem of fairly allocating a set of indivisible items among $n$ agents. Typically, the literature has focused on one-shot algorithms. In this talk we depart from this paradigm and allow items to arrive online. When an item arrives we must immediately and irrevocably allocate it to an agent. A paradigmatic example is that of food banks: food donations arrive, and must be delivered to nonprofit organizations such as food pantries and soup kitchens. Items are often perishable, which is why allocation decisions must be made quickly, and donated items are typically leftovers, leading to lack of information about items that will arrive in the future. Which recipient should a new donation go to? We approach this problem from different angles.</p>
<p>In the first part of the talk, we study the problem of minimizing the maximum envy between any two recipients, after all the goods have been allocated. We give a polynomial-time, deterministic and asymptotically optimal algorithm with vanishing envy, i.e. the maximum envy divided by the number of items T goes to zero as T goes to infinity. In the second part of the talk, we adopt and further develop an emerging paradigm called virtual democracy. We will take these ideas all the way to practice. In the last part of the talk I will present some results from an ongoing work on automating the decisions faced by a food bank called 412 Food Rescue, an organization in Pittsburgh that matches food donations with non-profit organizations.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Nov 5 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Nov 5</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><a href="http://www.fredpark.com/"><b>Fred Park</b></a><br />
Associate Professor<br />
Dept of Math &amp; Computer Science<br />
Whittier College</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Image Segmentation and Tracking Utilizing a Difference of Convex Regularized Mumford-Shah Functional</b></a></span></div><div class="togglec clearfix">In this talk I will give a brief overview of the segmentation and tracking problems and will propose a new model that tackles both of them. This model incorporates a weighted difference of anisotropic and isotropic total variation (TV) norms into a relaxed formulation of the Mumford-Shah (MS) model. We will show results exceeding those obtained by the MS model when using the standard TV norm to regularize partition boundaries. Examples illustrating the qualitative differences between the proposed model and the standard MS one will be shown as well. I will also talk about a fast numerical method that is used to optimize the proposed model utilizing the difference-of-convex algorithm (DCA) and the primal dual hybrid gradient (PDHG) method. Finally, future directions will be given that could harness the power of convolution nets for more advanced segmentation tasks.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Nov 12 =================================== --></p>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>Nov 12</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>No Seminar (Veterans Day)</b></div>
<p>&nbsp;</td>
</tr>
<p><!-- ==== Nov 19 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Nov 19</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><a href="https://ai.google/research/people/PhilipNelson"><b>Philip Nelson</b></a><br />
Director of Engineering<br />
Google Research</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Accelerating bio discovery with machine learning, the promise and the peril</b></a></span></div><div class="togglec clearfix">Google Accelerated Sciences is a translational research team that brings Google&#8217;s technological expertise to the scientific community.  Recent advances in machine learning have delivered incredible results in consumer applications (e.g. photo recognition, language translation), and is now beginning to play an important role in life sciences.  Taking examples from active collaborations in the biochemical, biological, and biomedical fields, I will focus on how our team transforms science problems into data problems and applies Google&#8217;s scaled computation, data-driven engineering, and machine learning to accelerate discovery. See <a href="http://g.co/research/gas">http://g.co/research/gas</a> for our publications and more details.</p>
<p><b>Bio:</b><br />Philip Nelson is a Director of Engineering in Google Research.  He joined Google in 2008 and was previously responsible for a range of Google applications and geo services.  In 2013, he helped found and currently leads the Google Accelerated Science team that collaborates with academic and commercial scientists to apply Google&#8217;s knowledge and experience and technologies to important scientific problems.  Philip graduated from MIT in 1985 where he did award-winning research on hip prosthetics at Harvard Medical School.  Before Google, Philip helped found and lead several Silicon Valley startups in search (Verity), optimization (Impresse), and genome sequencing (Complete Genomics) and was also an Entrepreneur in Residence at Accel Partners.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Nov 26 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Nov 26</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><a href="http://socsci.uci.edu/~rfutrell/"><b>Richard Futrell</b></a><br />
Assistant Professor<br />
Dept of Language Science<br />
UC Irvine</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Natural language as a code: Modeling human language using information theory</b></a></span></div><div class="togglec clearfix"><br />
Why is natural language the way it is? I propose that human languages can be modeled as solutions to the problem of efficient communication among intelligent agents with certain information processing constraints, in particular constraints on short-term memory. I present an analysis of dependency treebank corpora of over 50 languages showing that word orders across languages are optimized to limit short-term memory demands in parsing. Next I develop a Bayesian, information-theoretic model of human language processing, and show that this model can intuitively explain an apparently paradoxical class of comprehension errors made by both humans and state-of-the-art recurrent neural networks (RNNs). Finally I combine these insights in a model of human languages as information-theoretic codes for latent tree structures, and show that optimization of these codes for expressivity and compressibility results in grammars that resemble human languages.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Dec 3 =================================== --></p>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>Dec 3</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>No Seminar (NIPS)</b></div>
<p>&nbsp;</td>
</tr>
</tbody>
</table>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2018/09/fall-2018/" title="2:24 pm" rel="bookmark"><time class="entry-date genericon" datetime="2018-09-18T14:24:22+00:00">September 18, 2018</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-803" class="post-803 post type-post status-publish format-standard hentry category-news">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2018/08/two-new-nsf-awards-in-machine-learning-for-sameer-singh/" title="Permalink to Two new NSF awards in Machine Learning for Sameer Singh" rel="bookmark">Two new NSF awards in Machine Learning for Sameer Singh</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		<p><a href="http://cml.ics.uci.edu/wp-content/cml/uploads/singh.jpg"><img class="alignright size-full wp-image-647" src="http://cml.ics.uci.edu/wp-content/cml/uploads/singh.jpg" alt="" width="100" height="130" /></a>Congratulations to <a href="http://sameersingh.org/">Professor Sameer Singh</a> for receiving <a href="https://www.ics.uci.edu/community/news/view_news?id=1413">two multi-year research awards</a> from the National Science Foundation (NSF). Under the first grant, Sameer and his research team will develop new algorithms to better explain why classifiers make certain decisions, increasing user trust in such models. The second grant focuses on the development of new approached for extracting multimodal information from documents, such as text, images, numbers, and databases, with the goal of automatically creating new knowledge bases from relatively unstructured written documents.</p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2018/08/two-new-nsf-awards-in-machine-learning-for-sameer-singh/" title="5:38 pm" rel="bookmark"><time class="entry-date genericon" datetime="2018-08-29T17:38:16+00:00">August 29, 2018</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-778" class="post-778 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2018/04/spring-2018/" title="Permalink to Spring 2018" rel="bookmark">Spring 2018</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		<p><!-- ========================================================== --><br />
<!-- =====Spring Quarter ============================================== --><br />
<!-- ========================================================== --></p>
<table border="1" cellpadding="5">
<tbody>
<!-- ==== Apr 2 =================================== --></p>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>Apr 2</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>No Seminar</b></div>
</td>
</tr>
<p><!-- ==== Apr 9 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Apr 9</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><b>Sabino Miranda, Ph.D</b></div>
<div class="aiml-name">CONACyT Researcher</div>
<div class="aiml-name">Center for Research and Innovation in Information and Communication Technologies</div>
</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Towards a Multilingual and Error-Robust Approach for Sentiment Analysis</b></a></span></div><div class="togglec clearfix"><br />
Sentiment Analysis is a research area concerned with the computational analysis of people&#8217;s feelings or beliefs expressed in texts such as emotions, opinions, attitudes, appraisals, etc.  At the same time, with the growth of social media data (review websites, microblogging sites, etc.) on the Web, Twitter has received particular attention because it is a huge source of opinionated information with potential applications to decision-making tasks from business applications to the analysis of social and political events.  In this context, I will present the multilingual and error-robust approaches developed in our group to tackle sentiment analysis as a classification problem, mainly for informal written text such as Twitter. Our approaches have been tested in several benchmark contests such as SemEval (International Workshop on Semantic Evaluation),  TASS  (Workshop for Sentiment Analysis Focused on Spanish),  and PAN (Workshop on Digital Text Forensics).</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Apr 16 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Apr 16</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><b><a href="https://www.math.uci.edu/~rvershyn/index.html">Roman Vershynin</a></b></div>
<div class="aiml-name">Professor of Mathematics</div>
<div class="aiml-name">University of California, Irvine</div>
</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Boolean functions and random tensors</b></a></span></div><div class="togglec clearfix">A simple way to generate a Boolean function in n variables is to take the sign of some polynomial. Such functions are called polynomial threshold functions. How many low-degree polynomial threshold functions are there? This problem was solved for degree d=1 by Zuev in 1989 and has remained open for any higher degrees, including d=2, since then. In a joint work with Pierre Baldi (UCI), we settled the problem for all degrees d>1. The solution explores connections of Boolean functions to additive combinatorics and high-dimensional probability. This leads to a program of extending random matrix theory to random tensors, which is mostly an uncharted territory at present.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Apr 23 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Apr 23</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><b><a href="http://cs.brown.edu/people/ren/">Zhile Ren</a></b></div>
<div class="aiml-name">PhD Candidate, Computer Science</div>
<div class="aiml-name">Brown University</div>
</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Semantic Three-Dimensional Understanding of Dynamic Scenes</b></a></span></div><div class="togglec clearfix">We develop new representations and algorithms for three-dimensional (3D) scene understanding from images and videos. In cluttered indoor scenes, RGB-D images are typically described by local geometric features of the 3D point cloud. We introduce descriptors that account for 3D camera viewpoint, and use structured learning to perform 3D object detection and room layout prediction. We also extend this work by using latent support surfaces to capture style variations of 3D objects and help detect small objects. Contextual relationships among categories and layout are captured via a cascade of classifiers, leading to holistic scene hypotheses with improved accuracy. In outdoor autonomous driving applications, given two consecutive frames from a pair of stereo cameras, 3D scene flow methods simultaneously estimate the 3D geometry and motion of the observed scene. We incorporate semantic segmentation in a cascaded prediction framework to more accurately model moving objects by iteratively refining segmentation masks, stereo correspondences, 3D rigid motion estimates, and optical flow fields.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Apr 30 =================================== --></p>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>Apr 30</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>Cancelled</b></div>
</td>
</tr>
<p><!-- ==== May 7 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>May 7</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><b><a href="https://svivek.com/">Vivek Srikumar</a></b></div>
<div class="aiml-name">Assistant Professor</div>
<div class="aiml-name">University of Utah</div>
</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Natural Language Processing in the Wild: Opportunities & Challenges</b></a></span></div><div class="togglec clearfix">Natural language processing (NLP) sees potential applicability in a broad array of user-facing applications. To realize this potential, however, we need to address several challenges related to representations, data availability and scalability.</p>
<p>In this talk, I will discuss these concerns and how we may overcome them. First, as a motivating example of NLP&#8217;s broad reach, I will present our recent work on using language technology to improve mental health treatment. Then, I will focus on some of the challenges that need to be addressed. The choice of representations can make a big difference in our ability to reason about text; I will discuss recent work on developing rich semantic representations. Finally, I will touch upon the problem of systematically speeding up the entire NLP pipeline without sacrificing accuracy. As a concrete example, I will present a new algebraic characterization of the process of feature extraction, as a direct consequence of which, we can make trained classifiers significantly faster.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== May 14 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>May 14</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><b><a href="http://www.ics.uci.edu/~skong2/">Shu Kong</a></b></div>
<div class="aiml-name">PhD Candidate, Computer Science</div>
<div class="aiml-name">University of California, Irvine</div>
</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Pay Attention to the Pixel, Understand the Scene Better</b></a></span></div><div class="togglec clearfix">Objects may appear at arbitrary scales in perspective images of a scene, posing a challenge for recognition systems that process images at a fixed resolution. We propose a depth-aware gating module that adaptively selects the pooling field size (by fusing multi-scale pooled features) in a convolutional network architecture according to the object scale (inversely proportional to the depth) so that small details are preserved for distant objects while larger receptive fields are used for those nearby. The depth gating signal is provided by stereo disparity or estimated directly from monocular input. We further integrate this depth-aware gating into a recurrent convolutional neural network to refine semantic segmentation, and show state-of-the-art performance on several benchmarks.</p>
<p>Moreover, rather than fusing mutli-scale pooled features based on estimated depth, we show the &#8220;correct&#8221; size of pooling field for each pixel can be decided in an attentional fashion by our  Pixel-wise Attentional Gating unit (PAG), which learns to choose the pooling size for each pixel. PAG is a generic, architecture-independent, problem-agnostic mechanism that can be readily ‚Äúplugged in‚Äù to an existing model with fine-tuning. We utilize PAG in two ways: 1) learning spatially varying pooling fields that improves model performance without the extra computation cost, and 2) learning a dynamic computation policy for each pixel to decrease total computation while maintaining accuracy. We extensively evaluate PAG on a variety of per-pixel labeling tasks, including semantic segmentation, boundary detection, monocular depth and surface normal estimation. We demonstrate that PAG allows competitive or state-of-the-art performance on these tasks. We also show that PAG learns dynamic spatial allocation of computation over the input image which provides better performance trade-offs compared to related approaches (e.g., truncating deep models or dynamically skipping whole layers). Generally, we observe that PAG reduces computation by 10% without noticeable loss in accuracy, and performance degrades gracefully when imposing stronger computational constraints.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== May 21 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>May 21</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><b><a href="https://www.microsoft.com/en-us/research/people/rcaruana/">Rich Caruana</a></b></div>
<div class="aiml-name">Principal Researcher</div>
<div class="aiml-name">Microsoft Research</div>
</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Friends Don‚Äôt Let Friends Deploy Black-Box Models: The Importance of Intelligibility in Machine Learning</b></a></span></div><div class="togglec clearfix">In machine learning often a tradeoff must be made between accuracy and intelligibility: the most accurate models usually are not very intelligible (e.g., deep nets, boosted trees and random forests), and the most intelligible models usually are less accurate (e.g., logistic regression and decision lists).  This tradeoff often limits the accuracy of models that can be safely deployed in mission-critical applications such as healthcare where being able to understand, validate, edit, and ultimately trust a learned model is important.  We have been working on a learning method based on generalized additive models (GAMs) that is often as accurate as full complexity models, but even more intelligible than linear models.  This makes it easy to understand what a model has learned, and also makes it easier to edit the model when it learns inappropriate things because of unanticipated problems with the data.  Making it possible for experts to understand a model and repair it is critical because most data has unanticipated landmines.  In the talk I‚Äôll present two healthcare cases studies where these high-accuracy GAMs discover surprising patterns in the data that would have made deploying a black-box model risky.  I‚Äôll also briefly show how we‚Äôre using these models to detect bias in domains where fairness and transparency are paramount.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== May 28 =================================== --></p>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>May 28</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>Memorial Day</b></div>
</td>
</tr>
<p><!-- ==== Jun 4 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Jun 4</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><b>Stephen McAleer</b> (<a href="http://www.igb.uci.edu/~pfbaldi/">Pierre Baldi</a>&#8216;s group)</div>
<div class="aiml-name">Graduate Student, Computer Science</div>
<div class="aiml-name">University of California, Irvine</div>
</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Learning how to solve the Rubik's Cube with no Human Knowledge</b></a></span></div><div class="togglec clearfix">We will present a novel approach to solving the Rubik&#8217;s cube effectively without any human knowledge using several ingredients including deep learning, reinforcement learning, and Monte Carlo searches. </p>
<p>At the end, if time permits, we will describe several extensions to the neuronal Boolean complexity results presented by Roman Vershynin a few weeks ago.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Jun 11 =================================== --></p>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>Jun 11</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>No Seminar (finals week)</b></div>
</td>
</tr>
</tbody>
</table>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2018/04/spring-2018/" title="3:01 pm" rel="bookmark"><time class="entry-date genericon" datetime="2018-04-04T15:01:11+00:00">April 4, 2018</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-773" class="post-773 post type-post status-publish format-standard hentry category-news">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2018/03/workshop-for-the-philosophy-of-machine-learning/" title="Permalink to Workshop for the Philosophy of Machine Learning" rel="bookmark">Workshop for the Philosophy of Machine Learning</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		<p>UC Irvine held a very successful workshop on the &#8220;Philosophy of Machine Learning&#8221; on March 17th &amp; 18th, in the Donald Bren Hall Conference Center (DBH 6011). More information may be found at: <a href="https://philmachinelearning.wordpress.com/program/">https://philmachinelearning.wordpress.com/program/</a>.</p>
<p>Organizers: Andrew Holbrook (Statistics) and Kino Zhao (Logic and Philosophy of Science)</p>
<p>Sponsors: UCI School of Social Sciences; UCI Dept of Logic &amp; Philosophy of Science; UCI Data Science Initiative; and Dr. Babak Shahbaba (via NSF).</p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2018/03/workshop-for-the-philosophy-of-machine-learning/" title="4:13 pm" rel="bookmark"><time class="entry-date genericon" datetime="2018-03-13T16:13:07+00:00">March 13, 2018</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-747" class="post-747 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2018/01/winter-2018/" title="Permalink to Winter 2018" rel="bookmark">Winter 2018</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		<p><!-- ========================================================== --><br />
<!-- =====Winter Quarter ============================================== --><br />
<!-- ========================================================== --></p>
<table border="1" cellpadding="5"><!-- ==== Jan 15 =================================== --></p>
<tbody>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>Jan 15</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>No Seminar (MLK Day)</b></div>
<p>&nbsp;</td>
</tr>
<p><!-- ==== Jan 22 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Jan 22</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><b>Shufeng Kong</b></div>
<div class="aiml-name">PhD Candidate</div>
<div class="aiml-name">Centre for Quantum Software and Information, FEIT</div>
<div class="aiml-name">University of Technology Sydney, Australia</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Multiagent Simple Temporal Problem: The Arc-Consistency Approach</b></a></span></div><div class="togglec clearfix">The Simple Temporal Problem (STP) is a fundamental temporal<br />
reasoning problem and has recently been extended to<br />
the Multiagent Simple Temporal Problem (MaSTP). In this<br />
paper we present a novel approach that is based on enforcing<br />
arc-consistency (AC) on the input (multiagent) simple temporal<br />
network. We show that the AC-based approach is sufficient<br />
for solving both the STP and MaSTP and provide efficient<br />
algorithms for them. As our AC-based approach does<br />
not impose new constraints between agents, it does not violate<br />
the privacy of the agents and is superior to the state-ofthe-art<br />
approach to MaSTP. Empirical evaluations on diverse<br />
benchmark datasets also show that our AC-based algorithms<br />
for STP and MaSTP are significantly more efficient than existing<br />
approaches.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Jan 29 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Jan 29</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><b><a href="http://jiyfeng.github.io/">Yangfeng Ji</a></b></div>
<div class="aiml-name">Postdoctoral Scholar</div>
<div class="aiml-name">Paul Allen School of Computer Science and Engineering</div>
<div class="aiml-name">University of Washington</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Bringing Structural Information into Neural Network Design</b></a></span></div><div class="togglec clearfix">Deep learning is one of the most important techniques used in natural language processing (NLP). A central question in deep learning for NLP is how to design a neural network that can fully utilize the information from training data and make accurate predictions. A key to solving this problem is to design a better network architecture. </p>
<p>In this talk, I will present two examples from my work on how structural information from natural language helps design better neural network models. The first example shows adding coreference structures of entities not only helps different aspects of text modeling, but also improves the performance of language generation; the second example demonstrates structures of organizing sentences into coherent texts can help neural networks build better representations for various text classification tasks. Along the lines of this topic, I will also propose a few ideas for future work and discuss some potential challenges.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Feb 5 =================================== --></p>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>February 5</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>No Seminar (AAAI)</b></div>
<p>&nbsp;</td>
</tr>
<p><!-- ==== Feb 12 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>February 12</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><b><a href="https://www.ics.uci.edu/~enalisni/">Eric Nalisnick</a></b></div>
<div class="aiml-name">PhD Candidate</div>
<div class="aiml-name">Computer Science</div>
<div class="aiml-name">University of California, Irvine</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Averaging and Combining Variational Models with Stein Particle Descent</b></a></span></div><div class="togglec clearfix">Bayesian inference for complex models&#8212;the kinds needed to solve complex tasks such as object recognition&#8212;is inherently intractable, requiring analytically difficult integrals be solved in high dimensions.  One solution is to turn to variational Bayesian inference: a parametrized family of distributions is proposed, and optimization is carried out to find the member of the family nearest to the true posterior.  There is an innate trade-off within VI between expressive vs tractable approximations.  We wish the variational family to be as rich as possible so as it might include the true posterior (or something very close), but adding structure to the approximation increases the computational complexity of optimization.  As a result, there has been much interest in efficient optimization strategies for mixture model approximations.  In this talk, I&#8217;ll return to the problem of using mixture models for VI.  First, to motivate our approach, I&#8217;ll discuss the distinction between averaging vs combining variational models.  We show that optimization objectives aimed at fitting mixtures (i.e. model combination), in practice, are relaxed into performing something between model combination and averaging.  Our primary contribution is to formulate a novel training algorithm for variational model averaging by adapting Stein variational gradient descent to operate on the parameters of the approximating distribution.  Then, through a particular choice of kernel, we show the algorithm can be adapted to perform something closer to model combination, providing a new algorithm for optimizing (finite) mixture approximations.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Feb 19 =================================== --></p>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>February 19</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>No Seminar (President&#8217;s Day)</b></div>
<p>&nbsp;</td>
</tr>
<p><!-- ==== Feb 26 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>February 26</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><b><a href="https://www.jaypujara.org/">Jay Pujara</a></b></div>
<div class="aiml-name">Research Scientist</div>
<div class="aiml-name">ISI/USC</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>What do Probabilistic Models Know?</b></a></span></div><div class="togglec clearfix">Knowledge is an essential ingredient in the quest for artificial intelligence, yet scalable and robust approaches to acquiring knowledge have challenged AI researchers for decades. Often, the obstacle to knowledge acquisition is massive, uncertain, and changing data that obscures the underlying knowledge. In such settings, probabilistic models have excelled at exploiting the structure in the domain to overcome ambiguity, revise beliefs and produce interpretable results. In my talk, I will describe recent work using probabilistic models for knowledge graph construction and information extraction, including linking subjects across electronic health records, fusing background knowledge from scientific articles with gene association studies, disambiguating user browsing behavior across platforms and devices, and aligning structured data sources with textual summaries. I also highlight several areas of ongoing research, fusing embedding approaches with probabilistic modeling and building models that support dynamic data or human-in-the-loop interactions.</p>
<p><b>Bio:</b><br />
Jay Pujara is a research scientist at the University of Southern California&#8217;s Information Sciences Institute whose principal areas of research are machine learning, artificial intelligence, and data science. He completed a postdoc at UC Santa Cruz, earned his PhD at the University of Maryland, College Park and received his MS and BS at Carnegie Mellon University. Prior to his PhD, Jay spent six years at Yahoo! working on mail spam detection, user trust, and contextual mail experiences, and he has also worked at Google, LinkedIn and Oracle. Jay is the author of over thirty peer-reviewed publications and has received three best paper awards for his work. He is a recognized authority on knowledge graphs, and has organized the Automatic Knowledge Base Construction (AKBC) and Statistical Relational AI (StaRAI) workshops, has presented tutorials on knowledge graph construction at AAAI and WSDM, and has had his work featured in AI Magazine.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Mar 5 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>March 5</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><b><a href="http://www.cs.ucr.edu/~epapalex/">Vagelis Papalexakis</a></b></div>
<div class="aiml-name">Assistant Professor</div>
<div class="aiml-name">UC Riverside</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Tensor Decompositions for Big Multi-aspect Data Analytics</b></a></span></div><div class="togglec clearfix">Tensors and tensor decompositions have been very popular and effective tools for analyzing multi-aspect data in a wide variety of fields, ranging from Psychology to Chemometrics, and from Signal Processing to Data Mining and Machine Learning.  Using tensors in the era of big data presents us with a rich variety of applications, but also poses great challenges such as the one of scalability and efficiency. In this talk I will first motivate the effectiveness of tensor decompositions as data analytic tools in a variety of exciting, real-world applications. Subsequently, I will discuss recent techniques on tackling the scalability and efficiency challenges by parallelizing and speeding up tensor decompositions, especially for very sparse datasets, including the scenario where the data are continuously updated over time. Finally, I will discuss open problems in unsupervised tensor mining and quality assessment of the results, and present work-in-progress addressing that problem with very encouraging results.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Mar 12 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>March 12</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><b><a href="http://vision.ucla.edu/~alex/">Alessandro Achille</a></b></div>
<div class="aiml-name">PhD Student</div>
<div class="aiml-name">UC Los Angeles</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>The Emergence Theory of Deep Learning: Perception, Information Theory and PAC-Bayes</b></a></span></div><div class="togglec clearfix">I will describe the basic elements of the Emergence Theory of Deep Learning, that started as a general theory for representations, and is comprised of three parts: (1) We formalize the desirable properties that a representation should possess, based on classical principles of statistical decision and information theory: invariance, sufficiency, minimality, disentanglement. We then show that such an optimal representation of the data can be learned by minimizing a specific loss function which is related to the notion of Information Bottleneck and Variational Inference. (2) We analyze common empirical losses employed in Deep Learning (such as empirical cross-entropy), and implicit or explicit regularizers, including Dropout and Pooling, and show that they bias the network toward recovering such an optimal representation. Finally, (3) we show that minimizing a suitably (implicitly or explicitly) regularized loss with SGD with respect to the weights of the network implies implicit optimization of the loss described in (1), with relates instead to the activations of the network. Therefore, even when we optimize a DNN as a black-box classifier, we are always biased toward learning minimal, sufficient and invariant representation. The link between (implicit or explicit) regularization of the classification loss and learning of optimal representations is specific to the architecture of deep networks, and is not found in a general classifier. The theory is related to a new version of the Information Bottleneck that studies the weights of a network, rater than the activation, and can also be derived using PAC-Bayes or Kolmogorov complexity arguments, providing independent validation.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Mar 19 =================================== --></p>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>March 19</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>No Seminar (Finals Week)</b></div>
<p>&nbsp;</td>
</tr>
</tbody>
</table>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2018/01/winter-2018/" title="12:36 pm" rel="bookmark"><time class="entry-date genericon" datetime="2018-01-10T12:36:05+00:00">January 10, 2018</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
			
		
		</div><!-- #content -->
	</section><!-- #primary -->

	<div id="secondary" class="widget-area" role="complementary">
				<aside id="search-2" class="widget widget_search">	<form method="get" id="searchform" class="searchform" action="https://cml.ics.uci.edu/" role="search">
		<label for="s" class="screen-reader-text">Search</label>
		<input type="search" class="field" name="s" value="" id="s" placeholder="Search &hellip;" />
		<input type="submit" class="submit" id="searchsubmit" value="Search" />
	</form>
</aside>	</div><!-- #secondary -->

</div><!-- #page -->

<footer id="colophon" class="site-footer" role="contentinfo">
<p style="text-align:center;margin:0;">(c) 2015 <a href="http://cml.ics.uci.edu">Center for Machine Learning and Intelligent Systems</a>
	<div class="site-info">
				<a href="http://wordpress.org/" rel="generator">WordPress</a>/<a href="http://www.wpzoom.com/">BonPress</a>
	</div><!-- .site-info -->
</footer><!-- #colophon -->

<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-wpcom/js/navigation.js?ver=20120206'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-wpcom/js/skip-link-focus-fix.js?ver=20130115'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/wp-embed.min.js?ver=5.0.2'></script>

</body>
</html>
Mis_redirected¢DtypeEvalueıIhttp_code¢DtypeEvalue»Qdownload_complete¢DtypeEvalueı
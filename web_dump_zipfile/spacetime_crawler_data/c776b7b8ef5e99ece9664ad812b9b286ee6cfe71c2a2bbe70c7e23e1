¦Ifinal_url¡DtypeLhttp_headers¢DtypeEvalue‡¢Ak¢DtypeEvalueNContent-LengthAv¢DtypeEvalueD4584¢Ak¢DtypeEvalueMAccept-RangesAv¢DtypeEvalueEbytes¢Ak¢DtypeEvalueFServerAv¢DtypeEvalueX4Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips SVN/1.7.14¢Ak¢DtypeEvalueMLast-ModifiedAv¢DtypeEvalueXMon, 28 Jan 2019 17:37:21 GMT¢Ak¢DtypeEvalueDETagAv¢DtypeEvalueT"11e8-5808821087a44"¢Ak¢DtypeEvalueDDateAv¢DtypeEvalueXSun, 03 Feb 2019 02:22:58 GMT¢Ak¢DtypeEvalueLContent-TypeAv¢DtypeEvalueXtext/html; charset=UTF-8Kraw_content¢DtypeEvalueYè<!DOCTYPE html>
<html>
<title>Syllabus for CS 274A | Winter 2019</title>
<meta name="viewport" content="width=device-width, initial-scale=1">


<!--MATERIAL BELOW TO BE INCLUDED ON ALL SITE PAGES--> 
<link rel="stylesheet" href="https://www.ics.uci.edu/~smyth/test/localw3.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css"> <!--mini icons--> 
<link rel="stylesheet" href="http://www.w3schools.com/lib/w3-theme-blue.css">  <!-- color theme -->
<link rel="stylesheet" type="text/css" href="localstyle.css">  <!-- my own style -->
<link rel="stylesheet" type="text/css" href="https://www.ics.uci.edu/~smyth/courses/class_style.css">  <!-- Padhraic's class style --->  
<link rel="icon" type="image/png" href="https://news.uci.edu/wp-content/uploads/2014/12/favicon.png"/> <!-- favicon -->

<body class="w3-animate-opacity">
  
<div style="margin-left:10%;  margin-right:10%;  margin-top:30px">  
  

<!-- -------------------------------------------------------------------------------------------- -->
 
<h4> <span style="font-weight: bold;"> CS 274A: Syllabus and Schedule, Winter 2019 </span></h4> 

Note: the schedule may be adapted/updated during the quarter.
<br><br>
 
<ul>
<li> Week 1: January 7th
<ul>
<li>  <b>Probability Review</b>: random variables, conditional and
joint probabilities, Bayes rule, law of total probability,
chain rule and factorization. Sets of random variables, the
multivariate Gaussian model. Conditional independence and graphical
models.
</li>
</ul>
</li>
<br>

<li> Week 2: January 14th 
<ul> 
<li>  <b>Learning from Data</b>: Concepts of models and parameters. Definition of the 
likelihood function and the principle of maximum likelihood parameter estimation. 
</li>
<li> <b>Maximum Likelihood Learning</b>: Using maximum likelihood methods to learn the 
parameters of Gaussian models, binomial, multivariate and other parametric models.
</li>
</ul>
</li> 
<br>
 
<li> Week 3: January 21st
<ul>
<li> No lecture on Monday (university holiday)
</li>
<li>  <b>Sequence Models</b>: Learning from sequential data. Markov models and related approaches.
</li> 
</ul> </li> 
<br>

<li> Week 4: January 28th
<ul> 
<li>  <b>Bayesian Learning</b>: Frequentist and Bayesian views of probability. 
General principles of Bayesian estimation: prior densities, posterior densities, MAP, fully Bayesian approaches.  
</li> 
<li><b>Bayesian Learning</b>: Dirichlet/multinomial and Gaussian examples. Predictive densities, model selection, model averaging. 
</li> 
</ul> </li> 
<br>
 

<li> Week 5: February 4th
<ul>
<li> <b>Regression Learning I</b>: Linear models.  Probabilistic perspectives on regression. 
Loss functions. Parameter estimation methods for regression.   
</li>
<li>  <b>Regression Learning II</b>: Optimization algorithms, focusing on gradient
and stochastic gradient methods. Regularization and Bayesian methods.
Connections between regression and classification.
</li> 

</ul> </li> 
<br>

<li> Week 6: February 11th
<ul>
<li>  <b>Midterm Exam</b> during Monday's class (in-class, closed-book)
</li> 
<li>  <b>Bias-Variance Trade-offs</b>: The bias-variance trade-off for squared error and regression.
</li>

</ul> </li> 
<br>

<li> Week 7: February 18th
<ul>
<li> No lecture on Monday (university holiday)
</li>
<li> <b>Classification Learning</b>: Likelihood-based approaches and properties of objective functions. 
 Links between logistic regression and neural network models. 
</li> 
</ul> </li> 
<br>

<li> Week 8: February 25th
<ul>
<li> <b>Classification Learning</b>: Bayes rule, classification boundaries, discriminant functions,  
optimal decisions, Bayes error rate, Gaussian classifiers.
</li>
<li>  <b>Temporal Models</b>: Autoregressive models,  recurrent neural networks.

</li> 
</ul> </li> 
<br>


<li> Week 9: March 4th
<ul>
<li> <b>Mixture Models and EM</b>: Mixture models. Examples of mixture models for binary
and real-valued data.
The EM algorithm for learning Gaussian mixtures. 
 </li>
<li> <b>Mixture Models and EM</b>: Properties of the EM algorithm. Relation of K-means clustering to 
Gaussian mixture modeling. Mixtures for non-vector data.
</li> 
</ul> </li> 
<br>


<li> Week 10: March 11th
<ul>
<li> Monday: Additional topics in unsupervised learning</li>
<li> Wednesday: Additional topics in sequential models </li> 
</ul> </li> 
<br>


<li> Finals Week:  
<ul>
<li> <b>Final exam</b>, in class, Friday March 22nd,  8:00am to 10:00am.
The time of exam was selected by the registrar, not by the instructor :)
</li> 
</ul> </li> 

</ul>


</body>Mis_redirected¢DtypeEvalueôIhttp_code¢DtypeEvalueÈQdownload_complete¢DtypeEvalueõ
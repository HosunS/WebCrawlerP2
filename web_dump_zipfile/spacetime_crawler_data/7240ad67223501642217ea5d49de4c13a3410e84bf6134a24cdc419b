¶Ifinal_url¢DtypeEvaluexhttps://cml.ics.uci.edu/2017/Lhttp_headers¢DtypeEvalueá¢Ak¢DtypeEvalueQTransfer-EncodingAv¢DtypeEvalueGchunked¢Ak¢DtypeEvalueJKeep-AliveAv¢DtypeEvalueQtimeout=5, max=99¢Ak¢DtypeEvalueFServerAv¢DtypeEvalueX4Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips SVN/1.7.14¢Ak¢DtypeEvalueJConnectionAv¢DtypeEvalueJKeep-Alive¢Ak¢DtypeEvalueDLinkAv¢DtypeEvalueX<<https://cml.ics.uci.edu/wp-json/>; rel="https://api.w.org/"¢Ak¢DtypeEvalueDDateAv¢DtypeEvalueXMon, 18 Feb 2019 11:18:24 GMT¢Ak¢DtypeEvalueLContent-TypeAv¢DtypeEvalueXtext/html; charset=UTF-8Kraw_content¢DtypeEvalueYÏ«<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />
<title>2017 | Center for Machine Learning and Intelligent Systems</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://cml.ics.uci.edu/xmlrpc.php" />
<!--[if lt IE 9]>
<script src="https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-wpcom/js/html5.js" type="text/javascript"></script>
<![endif]-->

<link rel='dns-prefetch' href='//s.w.org' />
<link rel="alternate" type="application/rss+xml" title="Center for Machine Learning and Intelligent Systems &raquo; Feed" href="https://cml.ics.uci.edu/feed/" />
<link rel="alternate" type="application/rss+xml" title="Center for Machine Learning and Intelligent Systems &raquo; Comments Feed" href="https://cml.ics.uci.edu/comments/feed/" />
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/cml.ics.uci.edu\/wp-includes\/js\/wp-emoji-release.min.js?ver=5.0.2"}};
			!function(a,b,c){function d(a,b){var c=String.fromCharCode;l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,a),0,0);var d=k.toDataURL();l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,b),0,0);var e=k.toDataURL();return d===e}function e(a){var b;if(!l||!l.fillText)return!1;switch(l.textBaseline="top",l.font="600 32px Arial",a){case"flag":return!(b=d([55356,56826,55356,56819],[55356,56826,8203,55356,56819]))&&(b=d([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]),!b);case"emoji":return b=d([55358,56760,9792,65039],[55358,56760,8203,9792,65039]),!b}return!1}function f(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var g,h,i,j,k=b.createElement("canvas"),l=k.getContext&&k.getContext("2d");for(j=Array("flag","emoji"),c.supports={everything:!0,everythingExceptFlag:!0},i=0;i<j.length;i++)c.supports[j[i]]=e(j[i]),c.supports.everything=c.supports.everything&&c.supports[j[i]],"flag"!==j[i]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[j[i]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(h=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",h,!1),a.addEventListener("load",h,!1)):(a.attachEvent("onload",h),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),g=c.source||{},g.concatemoji?f(g.concatemoji):g.wpemoji&&g.twemoji&&(f(g.twemoji),f(g.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel='stylesheet' id='wp-block-library-css'  href='https://cml.ics.uci.edu/wp-includes/css/dist/block-library/style.min.css?ver=5.0.2' type='text/css' media='all' />
<link rel='stylesheet' id='bonpress-style-css'  href='https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-cml/style.css?ver=5.0.2' type='text/css' media='all' />
<link rel='stylesheet' id='tipsy-css'  href='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/css/tipsy.css?ver=5.0.2' type='text/css' media='all' />
<link rel='stylesheet' id='mts_wpshortcodes-css'  href='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/css/wp-shortcode.css?ver=5.0.2' type='text/css' media='all' />
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/jquery/jquery.js?ver=1.12.4'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/jquery/jquery-migrate.min.js?ver=1.4.1'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/js/jquery.tipsy.js?ver=5.0.2'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/js/wp-shortcode.js?ver=5.0.2'></script>
<link rel='https://api.w.org/' href='https://cml.ics.uci.edu/wp-json/' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://cml.ics.uci.edu/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://cml.ics.uci.edu/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 5.0.2" />
</head>

<body class="archive date group-blog">
<div id="page" class="hfeed site">
		<header id="masthead" class="all-header" role="banner">
		<hgroup class="hgroup-wide">
                        <a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home"><img src='/wp-content/cml/uploads/cml-curve.jpg'></a>
<!--			<h1 class="site-title"><a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home">Center for Machine Learning and Intelligent Systems</a></h1>
			<h2 class="site-description">University of California, Irvine</h2> -->
			<h1 class="site-title"><a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home">Center for Machine Learning and Intelligent Systems</a></h1>
			<h2 class="site-description">Bren School of Information and Computer Science</h2>			
			<h2 class="site-description">University of California, Irvine</h2>
			<div style="clear:both"></div>
		</hgroup>
	</header>
	<header id="masthead" class="site-header" role="banner">
		<hgroup class="hgroup-img">
                        <a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home"><img src='/wp-content/cml/uploads/cml-curve.jpg'></a>
			<h1 class="site-title"><a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home">Center for Machine Learning and Intelligent Systems</a></h1>
			<h2 class="site-description">University of California, Irvine</h2>
		</hgroup>

		<nav id="site-navigation" class="navigation-main" role="navigation">
			<h1 class="menu-toggle">Menu</h1>
			<div class="screen-reader-text skip-link"><a href="#content" title="Skip to content">Skip to content</a></div>

			<div class="menu-navigation-container"><ul id="menu-navigation" class="menu"><li id="menu-item-234" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-234"><a href="https://cml.ics.uci.edu/">Home</a></li>
<li id="menu-item-79" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-79"><a href="https://cml.ics.uci.edu/home/about-us/">About CML</a>
<ul class="sub-menu">
	<li id="menu-item-78" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-78"><a href="https://cml.ics.uci.edu/home/about-us/">About us</a></li>
	<li id="menu-item-429" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-429"><a href="https://cml.ics.uci.edu/category/news/">News</a></li>
	<li id="menu-item-76" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-76"><a href="https://cml.ics.uci.edu/home/contact-us/">Contact Us</a></li>
</ul>
</li>
<li id="menu-item-539" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-539"><a>People</a>
<ul class="sub-menu">
	<li id="menu-item-55" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-55"><a href="https://cml.ics.uci.edu/faculty/">Faculty</a></li>
	<li id="menu-item-220" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-220"><a href="https://cml.ics.uci.edu/alumni/">Alumni</a></li>
</ul>
</li>
<li id="menu-item-75" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-75"><a href="https://cml.ics.uci.edu/aiml/">Events &#038; Seminars</a>
<ul class="sub-menu">
	<li id="menu-item-74" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-74"><a href="https://cml.ics.uci.edu/aiml/">AI/ML Seminar Series</a></li>
	<li id="menu-item-73" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-73"><a href="https://cml.ics.uci.edu/aiml/ml-reading-group/">ML Reading Group</a></li>
</ul>
</li>
<li id="menu-item-222" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-222"><a>Education &#038; Resources</a>
<ul class="sub-menu">
	<li id="menu-item-227" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-227"><a href="https://cml.ics.uci.edu/courses/">Courses</a></li>
	<li id="menu-item-221" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-221"><a href="https://cml.ics.uci.edu/books/">Books</a></li>
</ul>
</li>
<li id="menu-item-81" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-81"><a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI Machine Learning Archive</a></li>
<li id="menu-item-87" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-87"><a href="https://cml.ics.uci.edu/sponsors-funding/">Sponsors &#038; Funding</a></li>
<li id="menu-item-86" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-86"><a href="https://cml.ics.uci.edu/subscribe/">Subscribe to CML List</a></li>
</ul></div>		</nav><!-- #site-navigation -->
	</header><!-- #masthead -->


	<section id="primary" class="content-area">
		<div id="content" class="site-content" role="main">

		
			<header class="page-header clear">
				<h1 class="page-title">
					<span>2017</span>				</h1>
							</header><!-- .page-header -->

						
				
<article id="post-731" class="post-731 post type-post status-publish format-standard hentry category-news">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2017/11/phd-students-win-best-poster-awards/" title="Permalink to PhD Students win Best Poster Awards" rel="bookmark">PhD Students win Best Poster Awards</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		<p>Congratulations to¬†CML graduate students for recent poster awards at the <a href="https://sites.google.com/view/socalml17/home">2017 Southern California Machine Learning Symposium</a> held at USC. ¬†Zhengli Zhao and Dheeru Dua (with advisor Sameer Singh) won best poster award for their work on generating natural adversarial examples and Eric Nalisnick (with advisor Padhraic Smyth) won honorable mention for his work on boosting variational inference. There were about 50 student posters presented and over 250 machine learning researchers attended the event. Next SoCal ML Symposium is scheduled for Fall 2018, to be hosted by UCLA.</p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2017/11/phd-students-win-best-poster-awards/" title="11:04 am" rel="bookmark"><time class="entry-date genericon" datetime="2017-11-14T11:04:14+00:00">November 14, 2017</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-724" class="post-724 post type-post status-publish format-standard hentry category-news">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2017/11/new-faculty-member-eric-sudderth/" title="Permalink to New Faculty Member: Erik Sudderth" rel="bookmark">New Faculty Member: Erik Sudderth</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		<p><a href="http://cml.ics.uci.edu/wp-content/cml/uploads/eriksudderthA.jpg"><img class="wp-image-726 alignleft" src="http://cml.ics.uci.edu/wp-content/cml/uploads/eriksudderthA-215x300.jpg" alt="" width="65" height="91" srcset="https://cml.ics.uci.edu/wp-content/cml/uploads/eriksudderthA-215x300.jpg 215w, https://cml.ics.uci.edu/wp-content/cml/uploads/eriksudderthA-768x1070.jpg 768w, https://cml.ics.uci.edu/wp-content/cml/uploads/eriksudderthA-735x1024.jpg 735w, https://cml.ics.uci.edu/wp-content/cml/uploads/eriksudderthA.jpg 1000w" sizes="(max-width: 65px) 100vw, 65px" /></a>We are delighted to welcome new faculty member Erik Sudderth to the Center. Erik recently joined the Department of Computer Science at UCI as an Associate Professor.¬†He is well-known for his research in machine learning, with interests in topics such as graphical models and Bayesian nonparametric methods. Erik&#8217;s research group is also active in the application of these ideas to artificial intelligence, vision, and the natural and social sciences. More information about Erik and his research group is available at <a href="http://www.ics.uci.edu/~sudderth/">Erik&#8217;s Webpage</a>.</p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2017/11/new-faculty-member-eric-sudderth/" title="10:48 am" rel="bookmark"><time class="entry-date genericon" datetime="2017-11-14T10:48:34+00:00">November 14, 2017</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-707" class="post-707 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2017/10/fall-2017/" title="Permalink to Fall 2017" rel="bookmark">Fall 2017</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		<p><!-- ========================================================== --><br />
<!-- =====Fall Quarter ============================================== --><br />
<!-- ========================================================== --></p>
<table cellpadding=5 border=1>
<col width="100">
<col>
<p>  <!-- ==== Oct 9 =================================== --></p>
<tr>
<td valign=top class='aiml-none'>
<div class="aiml-date"><b>Oct 9</b></div>
</td>
<td valign=top class='aiml-none'>
<div class="aiml-name"><b>No Seminar (Columbus Day)</b></div>
<p>
  </td>
</tr>
<p>  <!-- ==== Oct 16 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Oct 16</b><br />Bren Hall 3011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="https://baileykong.com/"><b>Bailey Kong</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Cross-Domain Forensic Shoeprint Matching</b></a></span></div><div class="togglec clearfix">We investigate the problem of automatically determining what type of shoe left an impression found at a crime scene. This recognition problem is made difficult by the variability in types of crime scene evidence (ranging from traces of dust or oil on hard surfaces to impressions made in soil) and the lack of comprehensive databases of shoe outsole tread patterns. We find that mid-level features extracted by pre-trained convolutional neural nets are surprisingly effective descriptors for these specialized domains. However, the choice of similarity measure for matching exemplars to a query image is essential to good performance.  For matching multi-channel deep features, we propose the use of multi-channel normalized cross-correlation and analyze its effectiveness. Finally, we introduce a discriminatively trained variant and fine-tune our system end-to-end, obtaining state-of-the-art performance.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Oct 23 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Oct 23</b><br />Bren Hall 3011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="https://www.linkedin.com/in/geng-ji/"><b>Geng Ji</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>From Patches to Images: A Nonparametric Generative Model</b></a></span></div><div class="togglec clearfix">We propose a hierarchical generative model that captures the self-similar structure of image regions as well as how this structure is shared across image collections. Our model is based on a novel, variational interpretation of the popular expected patch log-likelihood (EPLL) method as a model for randomly positioned grids of image patches. While previous EPLL methods modeled image patches with finite Gaussian mixtures, we use nonparametric Dirichlet process (DP) mixtures to create models whose complexity grows as additional images are observed. An extension based on the hierarchical DP then captures repetitive and self-similar structure via image-specific variations in cluster frequencies. We derive a structured variational inference algorithm that adaptively creates new patch clusters to more accurately model novel image textures. Our denoising performance on standard benchmarks is superior to EPLL and comparable to the state-of-the-art, and we provide novel statistical justifications for common image processing heuristics. We also show accurate image inpainting results.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Oct 30 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Oct 30</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www.ics.uci.edu/~qlou/"><b>Qi Lou</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Dynamic Importance Sampling for Anytime Bounds of the Partition Function</b></a></span></div><div class="togglec clearfix">Computing the partition function is a key inference task in many graphical models. In this paper, we propose a dynamic importance sampling scheme that provides  anytime finite-sample bounds for the partition function. Our algorithm balances the advantages of the three major inference strategies, heuristic search, variational bounds, and Monte Carlo methods, blending sampling with search to refine a variationally defined proposal. Our algorithm combines and generalizes recent work on anytime search and probabilistic bounds of the partition function. By using an intelligently chosen weighted average over the samples, we construct an unbiased estimator of the partition function with strong finite-sample confidence intervals that inherit both the rapid early improvement rate of sampling with the long-term benefits  of an improved proposal from search. This gives significantly improved anytime behavior, and more flexible trade-offs between memory, time, and solution quality. We demonstrate the effectiveness of our approach empirically on real-world problem instances taken from recent UAI competitions.<br /></div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Nov  6 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Nov  6</b><br />Bren Hall 3011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www.stat.washington.edu/vminin/"><b>Vladimir Minin</b></a><br />Professor<br />Department of Statistics<br />University of California, Irvine</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Advances of Bayesian nonparametrics in population genetics of infectious diseases</b></a></span></div><div class="togglec clearfix">Estimating evolutionary trees, called phylogenies or genealogies, is a fundamental task in modern biology. Once phylogenetic reconstruction is accomplished, scientists are faced with a challenging problem of interpreting phylogenetic trees. In certain situations, a coalescent process, a stochastic model that randomly generates evolutionary trees, comes to rescue by probabilistically connecting phylogenetic reconstruction with the demographic history of the population under study. An important application of the coalescent is phylodynamics, an area that aims at reconstructing past population dynamics from genomic data. Phylodynamic methods have been especially successful in analyses of genetic sequences from viruses circulating in human populations. From a Bayesian hierarchal modeling perspective, the coalescent process can be viewed as a prior for evolutionary trees, parameterized in terms of unknown demographic parameters, such as the population size trajectory. I will review Bayesian nonparametric techniques that can accomplish phylodynamic reconstruction, with a particular attention to analysis of genetic data sampled serially through time.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Nov 13 =================================== --><br />
<!--
  

<tr>
  

<td valign=top>

<div class="aiml-date"><b>Nov 13</b><br />Bren Hall 4011<br />1 pm</div>

</td>


  

<td valign=top>

<div class="aiml-name"><a href=""><b>tbd</b></a>

  </div>

<br />
  [toggle title="<a><b>tbd</b></a>"]tbd[/toggle]
  </td>


  </tr>


--></p>
<p>  <!-- ==== Nov 20 =================================== --></p>
<tr>
<td valign=top class='aiml-none'>
<div class="aiml-date"><b>Nov 20</b></div>
</td>
<td valign=top class='aiml-none'>
<div class="aiml-name"><b>No Seminar (Thanksgiving Week)</b></div>
<p>
  </td>
</tr>
<p>  <!-- ==== Nov 27 =================================== --><br />
<!--
  

<tr>
  

<td valign=top>

<div class="aiml-date"><b>Nov 27</b><br />Bren Hall 4011<br />1 pm</div>

</td>


  

<td valign=top>

<div class="aiml-name"><a href=""><b>tbd</b></a>

  </div>

<br />
  [toggle title="<a><b>tbd</b></a>"]tbd[/toggle]
  </td>


  </tr>


--></p>
<p>  <!-- ==== Dec  4 =================================== --></p>
<tr>
<td valign=top class='aiml-none'>
<div class="aiml-date"><b>Dec  4</b></div>
</td>
<td valign=top class='aiml-none'>
<div class="aiml-name"><b>No Seminar (NIPS Conference)</b></div>
<p>
  </td>
</tr>
<p>  <!-- ==== Dec  13 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Dec 13</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://http://yutianchen.com/"><b>Yutian Chen</b></a><br />Research Scientist<br />Google DeepMind</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Learning to Learn without Gradient Descent by Gradient Descent</b></a></span></div><div class="togglec clearfix">We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.</div></div><div class="clear"></div>
  </td>
</tr>
</table>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2017/10/fall-2017/" title="3:07 pm" rel="bookmark"><time class="entry-date genericon" datetime="2017-10-09T15:07:08+00:00">October 9, 2017</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-704" class="post-704 post type-post status-publish format-standard hentry category-news">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2017/06/singh-talk-oc-acm-chapter/" title="Permalink to Singh talk, OC ACM Chapter" rel="bookmark">Singh talk, OC ACM Chapter</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		<p>Center member Prof. Sameer Singh will discuss  his research on &#8220;Explaining Black-Box Machine Learning Predictions,&#8221; which addresses the important and challenging problem of enabling people to understand, predict and trust the behavior of machine learning models and algorithms. More information and online registration is available on the Orange County ACM Chapter <a href="http://ucidonaldbrenschoolofinformationandcomputersciences.cmail20.com/t/j-l-uduyzk-zhhjrltdt-y/">Meetup Event page</a>.</p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2017/06/singh-talk-oc-acm-chapter/" title="12:37 pm" rel="bookmark"><time class="entry-date genericon" datetime="2017-06-19T12:37:17+00:00">June 19, 2017</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-689" class="post-689 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2017/04/spring-2017/" title="Permalink to Spring 2017" rel="bookmark">Spring 2017</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		<br />
<table cellpadding=5 border=1>
<col width="100">
<col>
<p>  <!-- ==== Apr 10 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Apr 10</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="https://izbicki.me/research.html"><b>Mike Izbicki</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Riverside</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Divide & Conquer Techniques for Machine Learning</b></a></span></div><div class="togglec clearfix">I&#8217;ll present two algorithms that use divide and conquer techniques to speed up learning.  The first algorithm (called OWA) is a communication efficient distributed learner.  OWA uses only two rounds of communication, which is sufficient to achieve optimal learning rates.  The second algorithm is a meta-algorithm for fast cross validation.  I&#8217;ll show that for any divide and conquer learning algorithm, there exists a fast cross validation procedure whose run time is asymptotically independent of the number of cross validation folds.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Apr 17 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Apr 17</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www.ics.uci.edu/~jsupanci/"><b>James Supancic</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Long-Term Tracking by Decision Making</b></a></span></div><div class="togglec clearfix">Cameras can naturally capture sequences of images, or videos. And when understanding videos, connecting the past with the present requires tracking. Sometimes tracking is easy. We focus on two challenges which make tracking harder: long-term occlusions and appearance variations. To handle total occlusion, a tracker must know when it has lost track and how to reinitialize tracking when the target reappears. Reinitialization requires good appearance models. We build appearance models for humans and hands, with a particular emphasis on robustness and occlusion. For the second challenge, appearance variation, the tracker must know when and how to re-learn (or update) an appearance model. This challenge leads to the classic problem of drift: aggressively learning appearance changes allows small errors to compound, as elements of the background environment pollute the appearance model. We propose two solutions. First, we consider self-paced learning, wherein a tracker begins by learning from frames it finds easy. As the tracker becomes better at recognizing the target, it begins to learn from harder frames. We also develop a data-driven approach: train a tracking policy to decide when and how to update an appearance model. To take this direct approach to &#8220;learning when to learn&#8221;, we exploit large-scale Internet data through reinforcement learning. We interpret the resulting policy and conclude with a generalization for tracking multiple objects.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Apr 24 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Apr 24</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="https://ml.jpl.nasa.gov/people/thompson.shtml"><b>David R Thompson</b></a></p>
<p>Jet Propulsion Laboratory<br />California Institute of Technology</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Visible / Shortwave Infrared Imaging Spectroscopy at JPL: Instruments and Algorithms</b></a></span></div><div class="togglec clearfix">Imaging spectrometers enable quantitative maps of physical and chemical properties at high spatial resolution. They have a long history of deployments for mapping terrestrial and coastal aquatic ecosystems, geology, and atmospheric properties. They are also critical tools for exploring other planetary bodies. These high-dimensional spatio-spectral datasets pose a rich challenge for computer scientists and algorithm designers. This talk will provide an introduction to remote imaging spectroscopy in the Visible and Shortwave Infrared, describing the measurement strategy and data analysis considerations including atmospheric correction. We will describe historical and current instruments, software, and public datasets.</p>
<p><p><b>Bio:</b> David R. Thompson is a researcher and Technical Group Lead in the Imaging Spectroscopy group at the NASA Jet Propulsion Laboratory. He is Investigation Scientist for the AVIRIS imaging spectrometer project.  Other roles include software lead for the NEAScout mission, autonomy software lead for the PIXL instrument, and algorithm development for diverse JPL airborne imaging spectrometer campaigns. He is recipient of the NASA Early Career Achievement Medal and the JPL Lew Allen Award.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== May  1 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>May  1</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://faculty.sites.uci.edu/weinings/"><b>Weining Shen</b></a><br />Assistant Professor<br />Department of Statistics<br />University of California, Irvine</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Theory behind Bayesian nonparametrics</b></a></span></div><div class="togglec clearfix">Bayesian nonparametric (BNP) models have been widely used in modern applications. In this talk, I will discuss some recent theoretical results for the commonly used BNP methods from a frequentist asymptotic perspective. I will cover a set of function estimation and testing problems such as density estimation, high-dimensional partial linear regression, independence testing, and independent component analysis. Minimax optimal convergence rates, adaptation and Bernstein-von Mises theorem will be discussed.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== May  8 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>May  8</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="https://www.linkedin.com/in/p-anandan-72124b120/"><b>P. Anandan</b></a><br />VP for Research<br />Adobe Systems</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>How Data Science, Machine Learning, and AI are Transforming the Consumer Experience</b></a></span></div><div class="togglec clearfix">During the last two decades the experience of consumers has been undergoing a fundamental and dramatic transformation ‚Äì giving a rich variety of informed choices, online shopping, consumption of news and entertainment on the go, and personalized shopping experiences.  All of this has been powered by the massive amounts of data that is continuously being collected and the application of machine learning, data science and AI techniques to it.</p>
<p>Adobe is a leader the Digital Marketing and is the leading provider of solutions to enterprises that are serving customers both in the B2B and B2C space.  In this talk,  we will outline the current state of the industry and the technology that is behind it, how Data Science and Machine Learning are gradually beginning to transform the experiences of the consumer as well as the marketer. We will also speculate on how recent developments in Artificial Intelligence will lead to deep personalization  and richer experiences for the consumer as well as more powerful and tailored end-to-end capabilities for the marketer.</p>
<p><p><b>Bio:</b> Dr. P. Anandan is Vice President in Adobe Research, responsible for developing research strategy for Adobe, especially in Digital Marketing, and Leading the Adobe India Research lab. An emphasis of this lab is on Big Data Experience and Intelligence. At Adobe, he is also leading efforts in applying A.I. to Big Data. Dr. Anandan is an expert in Computer Vision with more than 60 publications that have earned 14,500 citations in Google Scholar. His research areas include visual motion analysis, video surveillance, and 3D scene modeling from images and video. His papers have won multiple awards including the Helmholtz Prize, for long term fundamental contributions to computer vision research. Prior to joining Adobe Dr. Anandan had a long tenure with Microsoft Research in Redmond, WA, and became a Distinguished Scientist. He was the Managing Director of Microsoft Research India, which he founded. Most recently he was the Managing Director of Microsoft Research‚Äôs Worldwide Outreach. He earned a PhD from the University of Massachusetts specializing in Computer Vision and Artificial Intelligence. He started as an assistant professor at Yale University before moving on to work in Video Information Processing at the David Sarnoff Research Center. His research has been used in DARPA‚Äôs Video Surveillance and Monitoring program as well as in creating special effects in the movies ‚ÄúWhat Dreams May Come‚Äù, ‚ÄúPrince of Egypt,‚Äù and ‚ÄúThe Matrix.‚Äù Dr. Anandan is the recipient of Distinguished Alumnus awards from both University of Massachusetts and the Indian Institute of Technology Madras, where he earned a B. Tech. in Electrical Engineering. He was inducted into the Nebraska Hall of Computing by the University of Nebraska, from where he obtained an MS in Computer Science. He is currently a member of the Board of Governors of IIT Madras.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== May 15 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>May 15</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://nakashole.com/"><b>Ndapa Nakashole</b></a><br />Assistant Professor<br />Computer Science and Engineering<br />University of California, San Diego</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Improving Zero-shot learning for word-level translation</b></a></span></div><div class="togglec clearfix">Zero-shot learning is used in computer vision, natural language, and other domains to induce mapping functions that project vectors from one vector space to another. This is a promising approach to learning, when we do not have labeled data for every possible label we want a system to recognize. This setting is common when doing NLP for low-resource languages, where labeled data is very scare.  In this talk, I will present our work on improving zero-shot learning methods for the task of word-level translation.</p>
<p><p><b>Bio:</b> Ndapa Nakashole is an Assistant Professor in the Department of Computer Science and Engineering at the University of California, San Diego. Prior to UCSD, she was a Postdoctoral Fellow in the Machine Learning Department at Carnegie Mellon University. She obtained her PhD from Saarland University, Germany, for work done at the Max Planck Institute for Informatics at Saarbr√ºcken.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== May 22 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>May 22</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href=""><b>Batya Kenig</b></a><br />Postdoctoral Scholar<br />Department of Information Systems Engineering<br />Technion &#8211; Israel Institute of Technology</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Querying Probabilistic Preferences in Databases</b></a></span></div><div class="togglec clearfix">We propose a novel framework wherein probabilistic preferences can be naturally represented and analyzed in a probabilistic relational database.  The framework augments the relational schema with a special type of a relation symbol, a preference symbol. A deterministic instance of this symbol holds a collection of binary relations.  Abstractly, the probabilistic variant is a probability space over databases of the augmented form (i.e., probabilistic database).  Effectively, each instance of a preference symbol can be represented as a collection of parametric preference distributions such as Mallows. We establish positive and negative complexity results for evaluating Conjunctive Queries (CQs) over databases where preferences are represented in the Repeated Insertion Model (RIM), Mallows being a special case. We show how CQ evaluation reduces to a novel inference problem (of independent interest) over RIM, and devise a solver with polynomial data complexity.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== May 29 =================================== --></p>
<tr>
<td valign=top class='aiml-none'>
<div class="aiml-date"><b>May 29</b></div>
</td>
<td valign=top class='aiml-none'>
<div class="aiml-name"><b>No Seminar (Memorial Day)</b></div>
<p>
  </td>
</tr>
<p>  <!-- ==== Jun  5 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Jun  5</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://yonatanbisk.com/"><b>Yonatan Bisk</b></a><br />Postdoctoral Scholar<br />Information Sciences Institute<br />University of Southern California</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>The Limits of Unsupervised Syntax and the Importance of Grounding in Language Acquisition</b></a></span></div><div class="togglec clearfix">The future of self-driving cars, personal robots, smart homes, and intelligent assistants hinges on our ability to communicate with computers. The failures and miscommunications of Siri-style systems are untenable and become more problematic as machines become more pervasive and are given more control over our lives. Despite the creation of massive proprietary datasets to train dialogue systems, these systems still fail at the most basic tasks. Further, their reliance on big data is problematic. First, successes in English cannot be replicated in most of the 6,000+ languages of the world. Second, while big data has been a boon for supervised training methods, many of the most interesting tasks will never have enough labeled data to actually achieve our goals. It is therefore important that we build systems which can learn from naturally occurring data and grounded situated interactions.</p>
<p>In this talk, I will discuss work from my thesis on the unsupervised acquisition of syntax which harnesses unlabeled text in over a dozen languages. This exploration leads us to novel insights into the limits of semantics-free language learning. Having isolated these stumbling blocks, I‚Äôll then present my recent work on language grounding where we attempt to learn the meaning of several linguistic constructions via interaction with the world.</p>
<p><p><b>Bio:</b> Yonatan Bisk‚Äôs research focuses on Natural Language Processing from naturally occurring data (unsupervised and weakly supervised data). He is a postdoc researcher with Daniel Marcu at USC‚Äôs Information Sciences Institute. Previously, he received his Ph.D. from the University of Illinois at Urbana-Champaign under Julia Hockenmaier and his BS from the University of Texas at Austin.</div></div><div class="clear"></div>
  </td>
</tr>
</table>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2017/04/spring-2017/" title="11:43 am" rel="bookmark"><time class="entry-date genericon" datetime="2017-04-05T11:43:49+00:00">April 5, 2017</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-667" class="post-667 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2017/01/winter-2017/" title="Permalink to Winter 2017" rel="bookmark">Winter 2017</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		<br />
<table cellpadding=5 border=1>
<col width="100">
<col>
<p>  <!-- ==== Jan 16 =================================== --></p>
<tr>
<td valign=top class='aiml-none'>
<div class="aiml-date"><b>Jan 16</b></div>
</td>
<td valign=top class='aiml-none'>
<div class="aiml-name"><b>No Seminar (MLK Day)</b></div>
<p>
  </td>
</tr>
<p>  <!-- ==== Jan 23 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Jan 23</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href=""><b>Mohammad Ghavamzadeh</b></a><br />Senior Analytics Researcher<br />Adobe Research</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Learning Safe Policies in Sequential Decision-Making Problems</b></a></span></div><div class="togglec clearfix">In online advertisement as well as many other fields such as health informatics and computational finance, we often have to deal with the situation in which we are given a batch of data generated by the current strategy(ies) of the company (hospital, investor), and we are asked to generate a good or an optimal strategy. Although there are many techniques to find a good policy given a batch of data, there are not much results to guarantee that the obtained policy will perform well in the real system without deploying it. On the other hand, deploying a policy might be risky, and thus, requires convincing the product (hospital, investment) manager that it is not going to harm the business. This is why it is extremely important to devise algorithms that generate policies with performance guarantees. </p>
<p> In this talk, we discuss four different approaches to this fundamental problem, we call them model-based, model-free, online, and risk-sensitive. In the model-based approach, we first use the batch of data and build a simulator that mimics the behavior of the dynamical system under studies (online advertisement, hospital‚Äôs ER, financial market), and then use this simulator to generate data and learn a policy. The main challenge here is to have guarantees on the performance of the learned policy, given the error in the simulator. This line of research is closely related to the area of robust learning and control. In the model-free approach, we learn a policy directly from the batch of data (without building a simulator), and the main question is whether the learned policy is guaranteed to perform at least as well as a baseline strategy. This line of research is related to off-policy evaluation and control. In the online approach, the goal is to control the exploration of the algorithm in a way that never during its execution the loss of using it instead of the baseline strategy is more than a given margin. In the risk-sensitive approach, the goal is to learn a policy that manages risk by minimizing some measure of variability in the performance in addition to maximizing a standard criterion. We present algorithms based on these approaches and demonstrate their usefulness in real-world applications such as personalized ad recommendation, energy arbitrage, traffic signal control, and American option pricing.</p>
<p><b>Bio:</b>Mohammad Ghavamzadeh received a Ph.D. degree in Computer Science from the University of Massachusetts Amherst in 2005. From 2005 to 2008, he was a postdoctoral fellow at the University of Alberta. He has been a permanent researcher at INRIA in France since November 2008. He was promoted to first-class researcher in 2010, was the recipient of the &#8220;INRIA award for scientific excellence&#8221; in 2011, and obtained his Habilitation in 2014. He is currently (from October 2013) on a leave of absence from INRIA working as a senior analytics researcher at Adobe Research in California, on projects related to digital marketing. He has been an area chair and a senior program committee member at NIPS, IJCAI, and AAAI. He has been on the editorial board of Machine Learning Journal (MLJ), has published over 50 refereed papers in major machine learning, AI, and control journals and conferences, and has organized several tutorials and workshops at NIPS, ICML, and AAAI. His research is mainly focused on sequential decision-making under uncertainty, reinforcement learning, and online learning.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Jan 27 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Jan 27</b><br />Bren Hall 6011<br />11:00am</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www.cs.cmu.edu/~rsalakhu/"><b>Ruslan Salakhutdinov</b></a><br />Associate Professor<br />Machine Learning Department<br />Carnegie Mellon University</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Learning Deep Unsupervised and Multimodal Models</b></a></span></div><div class="togglec clearfix">In this talk, I will first introduce a broad class of unsupervised deep learning models and show that they can learn useful hierarchical representations from large volumes of high-dimensional data with applications in information retrieval, object recognition, and speech perception. I will next introduce deep models that are capable of extracting a unified representation that fuses together multiple data modalities and present the Reverse Annealed Importance Sampling Estimator (RAISE) for evaluating these deep generative models. Finally, I will discuss models that can generate natural language descriptions (captions) of images and generate images from captions using attention, as well as introduce multiplicative and fine-grained gating mechanisms with application to reading comprehension.</p>
<p><p><b>Bio:</b> Ruslan Salakhutdinov received his PhD in computer science from the University of Toronto in 2009. After spending two post-doctoral years at the Massachusetts Institute of Technology Artificial Intelligence Lab, he joined the University of Toronto as an Assistant Professor in the Departments of Statistics and Computer Science. In 2016 he joined the Machine Learning Department at Carnegie Mellon University as an Associate Professor. Ruslan&#8217;s primary interests lie in deep learning, machine learning, and large-scale optimization. He is an action editor of the Journal of Machine Learning Research and served on the senior programme committee of several learning conferences including NIPS and ICML. He is an Alfred P. Sloan Research Fellow, Microsoft Research Faculty Fellow, Canada Research Chair in Statistical Machine Learning, a recipient of the Early Researcher Award, Google Faculty Award, Nvidia&#8217;s Pioneers of AI award, and is a Senior Fellow of the Canadian Institute for Advanced Research.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Jan 30 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Jan 30</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www.igb.uci.edu/~pfbaldi/"><b>Pierre Baldi &#038; Peter Sadowski</b></a><br />Chancellor&#8217;s Professor<br />Department of Computer Science<br />University of California, Irvine</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Learning in the Machine: Random Backpropagation and the Learning Channel</b></a></span></div><div class="togglec clearfix">Learning in the Machine is a style of machine learning that takes into account the physical constraints of learning machines, from brains to neuromorphic chips. Taking into account these constraints leads to new insights into the foundations of learning systems, and occasionally leads also to improvements for machine learning performed on digital computers. Learning in the Machine is particularly useful when applied to message passing algorithms such as backpropagation and belief propagation, and leads to the concepts of local learning and learning channel. These concepts in turn will be applied to random backpropagation and several new variants. In addition to simulations corroborating the remarkable robustness of these algorithms, we will present new mathematical results establishing interesting connections between machine learning and Hilbert 16th problem.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Feb  6 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Feb  6</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://itensor.org/miles/"><b>Miles Stoudenmire</b></a><br />Research Scientist<br />Department of Physics<br />University of California, Irvine</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Learning with Tensor Networks</b></a></span></div><div class="togglec clearfix">Tensor networks are a technique for factorizing tensors with hundreds or thousands of indices into a contracted network of low-order tensors. Originally developed at UCI in the 1990&#8217;s, tensor networks have revolutionized major areas of physics are starting to be used in applied math and machine learning. I will show that tensor networks fit naturally into a certain class of non-linear kernel learning models, such that advanced optimization techniques from physics can be applied straightforwardly (arxiv:1605.05775). I will discuss many advantages and future directions of tensor network models, for example adaptive pruning of weights and linear scaling with training set size (compared to at least quadratic scaling when using the kernel trick).</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Feb 13 =================================== --> </p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Feb 13</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www.ics.uci.edu/~qlou/"><b>Qi Lou</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Anytime Anyspace AND/OR Search for Bounding the Partition Function</b></a></span></div><div class="togglec clearfix">Bounding the partition function is a key inference task in many graphical models.  In this paper, we develop an anytime anyspace search algorithm taking advantage of AND/OR tree structure and optimized variational heuristics to tighten deterministic bounds on the partition function.  We study how our priority-driven best-first search scheme can improve on state-of-the-art variational bounds in an anytime way within limited memory resources, as well as the effect of the AND/OR framework to exploit conditional independence structure within the search process within the context of summation.  We compare our resulting bounds to a number of existing methods, and show that our approach offers a number of advantages on real-world problem instances taken from recent UAI competitions.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Feb 20 =================================== --></p>
<tr>
<td valign=top class='aiml-none'>
<div class="aiml-date"><b>Feb 20</b></div>
</td>
<td valign=top class='aiml-none'>
<div class="aiml-name"><b>No Seminar (Presidents Day)</b></div>
<p>
  </td>
</tr>
<p>  <!-- ==== Feb 27 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Feb 27</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="http://www.ics.uci.edu/~enalisni/"><b>Eric Nalisnick</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Deep Generative Models with Stick-Breaking Priors</b></a></span></div><div class="togglec clearfix">Deep generative models (such as the Variational Autoencoder) efficiently couple the expressiveness of deep neural networks with the robustness to uncertainty of probabilistic latent variables.  This talk will first give an overview of deep generative models, their applications, and approximate inference strategies for them.  Then I‚Äôll discuss our work on placing Bayesian Nonparametric priors on their latent space, which allows the hidden representations to grow as the data necessitates.</div></div><div class="clear"></div>
  </td>
</tr>
<p>  <!-- ==== Mar  6 =================================== --></p>
<tr>
<td valign=top>
<div class="aiml-date"><b>Mar  6</b><br />Bren Hall 4011<br />1 pm</div>
</td>
<td valign=top>
<div class="aiml-name"><a href="https://levyomer.wordpress.com/"><b>Omer Levy</b></a><br />Postdoctoral Researcher<br />Department of Computer Science &#038; Engineering<br />University of Washington</div>
<p>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Understanding Word Embeddings</b></a></span></div><div class="togglec clearfix">Neural word embeddings, such as word2vec (Mikolov et al., 2013), have become increasingly popular in both academic and industrial NLP. These methods attempt to capture the semantic meanings of words by processing huge unlabeled corpora with methods inspired by neural networks and the recent onset of Deep Learning. The result is a vectorial representation of every word in a low-dimensional continuous space. These word vectors exhibit interesting arithmetic properties (e.g. king &#8211; man + woman = queen) (Mikolov et al., 2013), and seemingly outperform traditional vector-space models of meaning inspired by Harris&#8217;s Distributional Hypothesis (Baroni et al., 2014). Our work attempts to demystify word embeddings, and understand what makes them so much better than traditional methods at capturing semantic properties.</p>
<p> Our main result shows that state-of-the-art word embeddings are actually &#8220;more of the same&#8221;. In particular, we show that skip-grams with negative sampling, the latest algorithm in word2vec, is implicitly factorizing a word-context PMI matrix, which has been thoroughly used and studied in the NLP community for the past 20 years. We also identify that the root of word2vec&#8217;s perceived superiority can be attributed to a collection of hyperparameter settings. While these hyperparameters were thought to be unique to neural-network inspired embedding methods, we show that they can, in fact, be ported to traditional distributional methods, significantly improving their performance. Among our qualitative results is a method for interpreting these seemingly-opaque word-vectors, and the answer to why king &#8211; man + woman = queen.</p>
<p> <b>Bio:</b> Omer Levy is a post-doc in the Department of Computer Science &#038; Engineering at the University of Washington, working with Prof. Luke Zettlemoyer. Previously, he completed his BSc and MSc at Technion ‚Äì Israel Institute of Technology with the guidance of Prof. Shaul Markovitch, and got his PhD at Bar-Ilan University with the supervision of Prof. Ido Dagan and Dr. Yoav Goldberg. Omer is interested in realizing high-level semantic applications such as question answering and summarization to help people cope with information overload. At the heart of these applications are challenges in textual entailment, semantic similarity, and reading comprehension, which form the core of my current research. He is also interested in the current advances in deep learning and how they can facilitate semantic applications.</div></div><div class="clear"></div>
  </td>
</tr>
</table>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2017/01/winter-2017/" title="4:41 pm" rel="bookmark"><time class="entry-date genericon" datetime="2017-01-18T16:41:36+00:00">January 18, 2017</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
			
		
		</div><!-- #content -->
	</section><!-- #primary -->

	<div id="secondary" class="widget-area" role="complementary">
				<aside id="search-2" class="widget widget_search">	<form method="get" id="searchform" class="searchform" action="https://cml.ics.uci.edu/" role="search">
		<label for="s" class="screen-reader-text">Search</label>
		<input type="search" class="field" name="s" value="" id="s" placeholder="Search &hellip;" />
		<input type="submit" class="submit" id="searchsubmit" value="Search" />
	</form>
</aside>	</div><!-- #secondary -->

</div><!-- #page -->

<footer id="colophon" class="site-footer" role="contentinfo">
<p style="text-align:center;margin:0;">(c) 2015 <a href="http://cml.ics.uci.edu">Center for Machine Learning and Intelligent Systems</a>
	<div class="site-info">
				<a href="http://wordpress.org/" rel="generator">WordPress</a>/<a href="http://www.wpzoom.com/">BonPress</a>
	</div><!-- .site-info -->
</footer><!-- #colophon -->

<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-wpcom/js/navigation.js?ver=20120206'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-wpcom/js/skip-link-focus-fix.js?ver=20130115'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/wp-embed.min.js?ver=5.0.2'></script>

</body>
</html>
Mis_redirected¢DtypeEvalueıIhttp_code¢DtypeEvalue»Qdownload_complete¢DtypeEvalueı